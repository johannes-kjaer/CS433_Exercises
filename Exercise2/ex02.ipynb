{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(e):\n",
    "    return e.T @ e / len(e)\n",
    "\n",
    "def MAE(e):\n",
    "    return np.sum(np.abs(e)) / len(e)\n",
    "\n",
    "def compute_loss(y, tx, w, costFunction=MSE):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    return costFunction(y - tx @ w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    for i in range(len(grid_w0)):\n",
    "        for j in range(len(grid_w1)):\n",
    "            losses[i,j] = compute_loss(y,tx,np.array([grid_w0[i],grid_w1[j]]).T)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=84.8489662935649, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.017 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    return - tx.T @ (y-tx @ w) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = np.zeros((max_iters+1,len(initial_w)))\n",
    "    ws[0] = initial_w\n",
    "    losses = np.zeros(max_iters)\n",
    "\n",
    "    for n in range(max_iters):\n",
    "        losses[n] = compute_loss(y,tx,ws[n])\n",
    "        grad_n = compute_gradient(y,tx,ws[n])\n",
    "        \n",
    "        ws[n+1] = ws[n] - grad_n * gamma\n",
    "\n",
    "        # store w and loss\n",
    "        print(f\"GD iter. {n+1}/{max_iters}: loss={losses[n]}, w={ws[n]}\")\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 1/50: loss=5584.47342551833, w=[0. 0.]\n",
      "GD iter. 2/50: loss=530.6049242179332, w=[51.3057454  9.4357987]\n",
      "GD iter. 3/50: loss=75.75675910088367, w=[66.69746902 12.26653832]\n",
      "GD iter. 4/50: loss=34.82042424034912, w=[71.31498611 13.1157602 ]\n",
      "GD iter. 5/50: loss=31.136154102900974, w=[72.70024123 13.37052676]\n",
      "GD iter. 6/50: loss=30.80456979053063, w=[73.11581777 13.44695673]\n",
      "GD iter. 7/50: loss=30.774727202417242, w=[73.24049073 13.46988572]\n",
      "GD iter. 8/50: loss=30.772041369487102, w=[73.27789262 13.47676442]\n",
      "GD iter. 9/50: loss=30.771799644523306, w=[73.28911319 13.47882803]\n",
      "GD iter. 10/50: loss=30.771777889276628, w=[73.29247936 13.47944711]\n",
      "GD iter. 11/50: loss=30.77177593130439, w=[73.29348921 13.47963284]\n",
      "GD iter. 12/50: loss=30.771775755086896, w=[73.29379216 13.47968856]\n",
      "GD iter. 13/50: loss=30.771775739227397, w=[73.29388305 13.47970527]\n",
      "GD iter. 14/50: loss=30.77177573780004, w=[73.29391032 13.47971029]\n",
      "GD iter. 15/50: loss=30.771775737671565, w=[73.2939185  13.47971179]\n",
      "GD iter. 16/50: loss=30.771775737659937, w=[73.29392095 13.47971224]\n",
      "GD iter. 17/50: loss=30.771775737658974, w=[73.29392169 13.47971238]\n",
      "GD iter. 18/50: loss=30.771775737658853, w=[73.29392191 13.47971242]\n",
      "GD iter. 19/50: loss=30.77177573765875, w=[73.29392197 13.47971243]\n",
      "GD iter. 20/50: loss=30.771775737658725, w=[73.29392199 13.47971243]\n",
      "GD iter. 21/50: loss=30.771775737658896, w=[73.293922   13.47971243]\n",
      "GD iter. 22/50: loss=30.7717757376588, w=[73.293922   13.47971243]\n",
      "GD iter. 23/50: loss=30.77177573765875, w=[73.293922   13.47971243]\n",
      "GD iter. 24/50: loss=30.771775737658782, w=[73.293922   13.47971243]\n",
      "GD iter. 25/50: loss=30.77177573765875, w=[73.293922   13.47971243]\n",
      "GD iter. 26/50: loss=30.7717757376588, w=[73.293922   13.47971243]\n",
      "GD iter. 27/50: loss=30.771775737658796, w=[73.293922   13.47971243]\n",
      "GD iter. 28/50: loss=30.7717757376588, w=[73.293922   13.47971243]\n",
      "GD iter. 29/50: loss=30.771775737658736, w=[73.293922   13.47971243]\n",
      "GD iter. 30/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 31/50: loss=30.771775737658817, w=[73.293922   13.47971243]\n",
      "GD iter. 32/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 33/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 34/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 35/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 36/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 37/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 38/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 39/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 40/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 41/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 42/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 43/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 44/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 45/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 46/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 47/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 48/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 49/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD iter. 50/50: loss=30.771775737658807, w=[73.293922   13.47971243]\n",
      "GD: execution time=0.033 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9918faf8de5d4d65ba7cbcd02b4adc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "def compute_stoch_gradient(y,tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    return - tx.T @ (y - tx @ w) / len(y)\n",
    "\n",
    "def mini_batch(y,tx,batch_size):\n",
    "    shuffledIndexes = np.random.permutation(len(y))\n",
    "    return y[shuffledIndexes[0:batch_size]] , tx[shuffledIndexes[0:batch_size]]\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = np.zeros((max_iters+1,len(initial_w)))\n",
    "    ws[0] = initial_w\n",
    "    losses = np.zeros(max_iters)\n",
    "\n",
    "    for n in range(max_iters):\n",
    "        losses[n] = compute_loss(y,tx,ws[n])\n",
    "\n",
    "        batch_y, batch_x = mini_batch(y,tx,batch_size)\n",
    "        ws[n+1] = ws[n] - gamma * compute_stoch_gradient(batch_y,batch_x,ws[n])\n",
    "\n",
    "        print(f\"SGD iter. {n+1}/{max_iters}: loss={losses[n]}, w={ws[n]}\")\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 1/50: loss=5739.670229071705, w=[0. 0.]\n",
      "SGD iter. 2/50: loss=5074.656381579685, w=[ 5.69802145 -5.34703121]\n",
      "SGD iter. 3/50: loss=3811.9203369768243, w=[14.05310765  2.18645568]\n",
      "SGD iter. 4/50: loss=2914.47493898687, w=[21.32567038 10.0964788 ]\n",
      "SGD iter. 5/50: loss=2334.964374686377, w=[27.22620558 14.02945631]\n",
      "SGD iter. 6/50: loss=1916.385411220256, w=[32.08967946 15.76362146]\n",
      "SGD iter. 7/50: loss=1588.6972215467563, w=[36.00005467 13.80655166]\n",
      "SGD iter. 8/50: loss=1263.353339789872, w=[40.72175868 15.45451218]\n",
      "SGD iter. 9/50: loss=1073.383179496354, w=[43.50582899 13.77118602]\n",
      "SGD iter. 10/50: loss=925.933627033059, w=[46.51055862 16.92304328]\n",
      "SGD iter. 11/50: loss=808.622917483971, w=[48.94658943 17.79403258]\n",
      "SGD iter. 12/50: loss=755.3204410551934, w=[50.82247964 20.15157135]\n",
      "SGD iter. 13/50: loss=610.9974593294621, w=[53.20354672 17.65446358]\n",
      "SGD iter. 14/50: loss=494.85293515736623, w=[55.39708972 14.82907909]\n",
      "SGD iter. 15/50: loss=439.93340746038473, w=[56.85061396 14.44667601]\n",
      "SGD iter. 16/50: loss=383.9555370934191, w=[58.29230922 12.831498  ]\n",
      "SGD iter. 17/50: loss=327.0694694905248, w=[60.56618317 14.62852732]\n",
      "SGD iter. 18/50: loss=299.64040241735177, w=[61.68645445 14.84029624]\n",
      "SGD iter. 19/50: loss=259.0228791291917, w=[63.21712724 14.10477963]\n",
      "SGD iter. 20/50: loss=251.228730076954, w=[65.61028083 17.95136365]\n",
      "SGD iter. 21/50: loss=242.37617313362492, w=[66.18120701 17.98588556]\n",
      "SGD iter. 22/50: loss=229.00324584147035, w=[67.05078975 17.95611373]\n",
      "SGD iter. 23/50: loss=247.359881512805, w=[68.44154118 20.19150816]\n",
      "SGD iter. 24/50: loss=240.5436698504628, w=[68.14578806 19.61463349]\n",
      "SGD iter. 25/50: loss=251.40707428016353, w=[67.31117877 19.6310338 ]\n",
      "SGD iter. 26/50: loss=249.65820826004384, w=[67.09644683 19.35336236]\n",
      "SGD iter. 27/50: loss=227.1924188882589, w=[67.79334277 18.51567501]\n",
      "SGD iter. 28/50: loss=222.1707286631712, w=[68.10423119 18.43388437]\n",
      "SGD iter. 29/50: loss=226.18591580272494, w=[68.60863125 19.06744541]\n",
      "SGD iter. 30/50: loss=223.8317002036236, w=[69.01208193 19.18412155]\n",
      "SGD iter. 31/50: loss=214.33496226441588, w=[67.98493951 17.77820556]\n",
      "SGD iter. 32/50: loss=190.32478659339583, w=[68.94990917 16.71560432]\n",
      "SGD iter. 33/50: loss=173.34154223050618, w=[69.89570687 15.94138772]\n",
      "SGD iter. 34/50: loss=160.5920180873033, w=[70.91122189 15.36693083]\n",
      "SGD iter. 35/50: loss=160.88033967343836, w=[71.00802413 15.46847303]\n",
      "SGD iter. 36/50: loss=161.3915877244072, w=[70.96451342 15.49580325]\n",
      "SGD iter. 37/50: loss=168.6775314868041, w=[71.75988293 16.64645107]\n",
      "SGD iter. 38/50: loss=167.8381364054334, w=[71.66265578 16.52961318]\n",
      "SGD iter. 39/50: loss=166.59742386753575, w=[71.74555283 16.45182246]\n",
      "SGD iter. 40/50: loss=159.5293095000139, w=[71.10338265 15.38001217]\n",
      "SGD iter. 41/50: loss=162.3576912792872, w=[71.75686181 16.05044755]\n",
      "SGD iter. 42/50: loss=160.82172769115974, w=[71.56056978 15.79662089]\n",
      "SGD iter. 43/50: loss=162.98007048425606, w=[71.39510228 15.93134929]\n",
      "SGD iter. 44/50: loss=161.15857863578728, w=[71.73626139 15.91966905]\n",
      "SGD iter. 45/50: loss=163.81880804156634, w=[71.57766815 16.10998719]\n",
      "SGD iter. 46/50: loss=166.27096656835639, w=[71.41945716 16.26899035]\n",
      "SGD iter. 47/50: loss=151.47166769391822, w=[72.46617879 15.16345372]\n",
      "SGD iter. 48/50: loss=142.7061133555654, w=[72.99158808 14.14719441]\n",
      "SGD iter. 49/50: loss=144.49770386850977, w=[73.39826852 14.52602125]\n",
      "SGD iter. 50/50: loss=168.85135532009363, w=[74.83424079 17.06834103]\n",
      "SGD: execution time=0.003 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94d123137e34dcc81cb78ff2cd2d4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 1/50: loss=5739.670229071705, w=[0. 0.]\n",
      "GD iter. 2/50: loss=636.5642494031929, w=[51.8474641   7.72442641]\n",
      "GD iter. 3/50: loss=177.28471123302563, w=[67.40170333 10.04175433]\n",
      "GD iter. 4/50: loss=135.94955279771045, w=[72.0679751 10.7369527]\n",
      "GD iter. 5/50: loss=132.22938853853208, w=[73.46785663 10.94551222]\n",
      "GD iter. 6/50: loss=131.89457375520604, w=[73.88782109 11.00808007]\n",
      "GD iter. 7/50: loss=131.8644404247067, w=[74.01381042 11.02685043]\n",
      "GD iter. 8/50: loss=131.86172842496174, w=[74.05160723 11.03248153]\n",
      "GD iter. 9/50: loss=131.8614843449847, w=[74.06294627 11.03417087]\n",
      "GD iter. 10/50: loss=131.86146237778678, w=[74.06634798 11.03467767]\n",
      "GD iter. 11/50: loss=131.86146040073896, w=[74.06736849 11.03482971]\n",
      "GD iter. 12/50: loss=131.86146022280465, w=[74.06767465 11.03487532]\n",
      "GD iter. 13/50: loss=131.86146020679058, w=[74.06776649 11.034889  ]\n",
      "GD iter. 14/50: loss=131.8614602053493, w=[74.06779405 11.03489311]\n",
      "GD iter. 15/50: loss=131.86146020521957, w=[74.06780231 11.03489434]\n",
      "GD iter. 16/50: loss=131.8614602052079, w=[74.06780479 11.03489471]\n",
      "GD iter. 17/50: loss=131.86146020520687, w=[74.06780554 11.03489482]\n",
      "GD iter. 18/50: loss=131.86146020520678, w=[74.06780576 11.03489485]\n",
      "GD iter. 19/50: loss=131.86146020520673, w=[74.06780583 11.03489486]\n",
      "GD iter. 20/50: loss=131.86146020520675, w=[74.06780585 11.03489486]\n",
      "GD iter. 21/50: loss=131.86146020520678, w=[74.06780585 11.03489487]\n",
      "GD iter. 22/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 23/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 24/50: loss=131.86146020520675, w=[74.06780585 11.03489487]\n",
      "GD iter. 25/50: loss=131.86146020520675, w=[74.06780585 11.03489487]\n",
      "GD iter. 26/50: loss=131.86146020520675, w=[74.06780585 11.03489487]\n",
      "GD iter. 27/50: loss=131.86146020520678, w=[74.06780585 11.03489487]\n",
      "GD iter. 28/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 29/50: loss=131.86146020520675, w=[74.06780585 11.03489487]\n",
      "GD iter. 30/50: loss=131.86146020520678, w=[74.06780585 11.03489487]\n",
      "GD iter. 31/50: loss=131.86146020520675, w=[74.06780585 11.03489487]\n",
      "GD iter. 32/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 33/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 34/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 35/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 36/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 37/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 38/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 39/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 40/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 41/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 42/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 43/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 44/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 45/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 46/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 47/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 48/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 49/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD iter. 50/50: loss=131.86146020520673, w=[74.06780585 11.03489487]\n",
      "GD: execution time=0.003 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "losses, ws = gradient_descent(y,tx,w_initial,max_iters,gamma)\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1730da08c6448db1fc513e7a85a0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    e = y - tx@w\n",
    "    return - np.sum(np.sign(e) * tx.T,axis=1) / len(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    losses = np.zeros(max_iters)\n",
    "    ws = np.zeros((max_iters+1,2))\n",
    "    ws[0] = initial_w\n",
    "\n",
    "    for n in range(max_iters):\n",
    "        losses[n] = compute_loss(y,tx,ws[n],MAE)\n",
    "        ws[n+1] = ws[n] - gamma * compute_subgradient_mae(y,tx,ws[n])\n",
    "       \n",
    "        print(f\"SubGD iter. {n+1}/{max_iters}: loss={losses[n]}, w={ws[n]}\")\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 1/500: loss=74.06780585492638, w=[0. 0.]\n",
      "SubGD iter. 2/500: loss=73.36780585492637, w=[7.00000000e-01 6.10952433e-16]\n",
      "SubGD iter. 3/500: loss=72.66780585492637, w=[1.40000000e+00 1.22190487e-15]\n",
      "SubGD iter. 4/500: loss=71.96780585492638, w=[2.1000000e+00 1.8328573e-15]\n",
      "SubGD iter. 5/500: loss=71.26780585492638, w=[2.80000000e+00 2.44380973e-15]\n",
      "SubGD iter. 6/500: loss=70.56780585492638, w=[3.50000000e+00 3.05476216e-15]\n",
      "SubGD iter. 7/500: loss=69.86780585492637, w=[4.2000000e+00 3.6657146e-15]\n",
      "SubGD iter. 8/500: loss=69.16780585492639, w=[4.90000000e+00 4.27666703e-15]\n",
      "SubGD iter. 9/500: loss=68.46780585492637, w=[5.60000000e+00 4.88761946e-15]\n",
      "SubGD iter. 10/500: loss=67.76780585492638, w=[6.30000000e+00 5.49857189e-15]\n",
      "SubGD iter. 11/500: loss=67.06780585492638, w=[7.00000000e+00 6.10952433e-15]\n",
      "SubGD iter. 12/500: loss=66.36780585492637, w=[7.70000000e+00 6.72047676e-15]\n",
      "SubGD iter. 13/500: loss=65.66780585492639, w=[8.40000000e+00 7.33142919e-15]\n",
      "SubGD iter. 14/500: loss=64.96780585492638, w=[9.10000000e+00 7.94238163e-15]\n",
      "SubGD iter. 15/500: loss=64.26780585492638, w=[9.80000000e+00 8.55333406e-15]\n",
      "SubGD iter. 16/500: loss=63.567805854926384, w=[1.05000000e+01 9.16428649e-15]\n",
      "SubGD iter. 17/500: loss=62.867805854926374, w=[1.12000000e+01 9.77523892e-15]\n",
      "SubGD iter. 18/500: loss=62.167805854926385, w=[1.19000000e+01 1.03861914e-14]\n",
      "SubGD iter. 19/500: loss=61.46780585492638, w=[1.26000000e+01 1.09971438e-14]\n",
      "SubGD iter. 20/500: loss=60.76780585492638, w=[1.33000000e+01 1.16080962e-14]\n",
      "SubGD iter. 21/500: loss=60.067805854926384, w=[1.40000000e+01 1.22190487e-14]\n",
      "SubGD iter. 22/500: loss=59.36780585492639, w=[1.47000000e+01 1.28300011e-14]\n",
      "SubGD iter. 23/500: loss=58.667805854926385, w=[1.54000000e+01 1.34409535e-14]\n",
      "SubGD iter. 24/500: loss=57.96780585492638, w=[1.6100000e+01 1.4051906e-14]\n",
      "SubGD iter. 25/500: loss=57.26780585492638, w=[1.68000000e+01 1.46628584e-14]\n",
      "SubGD iter. 26/500: loss=56.567805854926384, w=[1.75000000e+01 1.52738108e-14]\n",
      "SubGD iter. 27/500: loss=55.867805854926395, w=[1.82000000e+01 1.58847633e-14]\n",
      "SubGD iter. 28/500: loss=55.167805854926385, w=[1.89000000e+01 1.64957157e-14]\n",
      "SubGD iter. 29/500: loss=54.46780585492638, w=[1.96000000e+01 1.71066681e-14]\n",
      "SubGD iter. 30/500: loss=53.767805854926394, w=[2.03000000e+01 1.77176206e-14]\n",
      "SubGD iter. 31/500: loss=53.06780585492639, w=[2.1000000e+01 1.8328573e-14]\n",
      "SubGD iter. 32/500: loss=52.367805854926395, w=[2.17000000e+01 1.89395254e-14]\n",
      "SubGD iter. 33/500: loss=51.667805854926385, w=[2.24000000e+01 1.95504778e-14]\n",
      "SubGD iter. 34/500: loss=50.96780585492639, w=[2.31000000e+01 2.01614303e-14]\n",
      "SubGD iter. 35/500: loss=50.267805854926394, w=[2.38000000e+01 2.07723827e-14]\n",
      "SubGD iter. 36/500: loss=49.56780585492639, w=[2.45000000e+01 2.13833351e-14]\n",
      "SubGD iter. 37/500: loss=48.867805854926395, w=[2.52000000e+01 2.19942876e-14]\n",
      "SubGD iter. 38/500: loss=48.1678058549264, w=[2.590000e+01 2.260524e-14]\n",
      "SubGD iter. 39/500: loss=47.4678058549264, w=[2.66000000e+01 2.32161924e-14]\n",
      "SubGD iter. 40/500: loss=46.7678058549264, w=[2.73000000e+01 2.38271449e-14]\n",
      "SubGD iter. 41/500: loss=46.067805854926405, w=[2.80000000e+01 2.44380973e-14]\n",
      "SubGD iter. 42/500: loss=45.367805854926395, w=[2.87000000e+01 2.50490497e-14]\n",
      "SubGD iter. 43/500: loss=44.6678058549264, w=[2.94000000e+01 2.56600022e-14]\n",
      "SubGD iter. 44/500: loss=43.9678058549264, w=[3.01000000e+01 2.62709546e-14]\n",
      "SubGD iter. 45/500: loss=43.2678058549264, w=[3.0800000e+01 2.6881907e-14]\n",
      "SubGD iter. 46/500: loss=42.567805854926405, w=[3.15000000e+01 2.74928595e-14]\n",
      "SubGD iter. 47/500: loss=41.867805854926395, w=[3.22000000e+01 2.81038119e-14]\n",
      "SubGD iter. 48/500: loss=41.167805854926385, w=[3.29000000e+01 2.87147643e-14]\n",
      "SubGD iter. 49/500: loss=40.46780585492639, w=[3.36000000e+01 2.93257168e-14]\n",
      "SubGD iter. 50/500: loss=39.767805854926394, w=[3.43000000e+01 2.99366692e-14]\n",
      "SubGD iter. 51/500: loss=39.067805854926384, w=[3.50000000e+01 3.05476216e-14]\n",
      "SubGD iter. 52/500: loss=38.36780585492638, w=[3.57000000e+01 3.11585741e-14]\n",
      "SubGD iter. 53/500: loss=37.66780585492638, w=[3.64000000e+01 3.17695265e-14]\n",
      "SubGD iter. 54/500: loss=36.96780585492638, w=[3.71000000e+01 3.23804789e-14]\n",
      "SubGD iter. 55/500: loss=36.26780585492637, w=[3.78000000e+01 3.29914314e-14]\n",
      "SubGD iter. 56/500: loss=35.56780585492637, w=[3.85000000e+01 3.36023838e-14]\n",
      "SubGD iter. 57/500: loss=34.86780585492637, w=[3.92000000e+01 3.42133362e-14]\n",
      "SubGD iter. 58/500: loss=34.16780585492637, w=[3.99000000e+01 3.48242887e-14]\n",
      "SubGD iter. 59/500: loss=33.46780585492636, w=[4.06000000e+01 3.54352411e-14]\n",
      "SubGD iter. 60/500: loss=32.767805854926365, w=[4.13000000e+01 3.60461935e-14]\n",
      "SubGD iter. 61/500: loss=32.067805854926355, w=[4.2000000e+01 3.6657146e-14]\n",
      "SubGD iter. 62/500: loss=31.36780585492636, w=[4.27000000e+01 3.72680984e-14]\n",
      "SubGD iter. 63/500: loss=30.66780585492635, w=[4.34000000e+01 3.78790508e-14]\n",
      "SubGD iter. 64/500: loss=29.967805854926347, w=[4.41000000e+01 3.84900033e-14]\n",
      "SubGD iter. 65/500: loss=29.267805854926348, w=[4.48000000e+01 3.91009557e-14]\n",
      "SubGD iter. 66/500: loss=28.567805854926345, w=[4.55000000e+01 3.97119081e-14]\n",
      "SubGD iter. 67/500: loss=27.867805854926342, w=[4.62000000e+01 4.03228606e-14]\n",
      "SubGD iter. 68/500: loss=27.17327020966892, w=[4.6900000e+01 4.0933813e-14]\n",
      "SubGD iter. 69/500: loss=26.490451563751197, w=[4.75930693e+01 1.11478457e-02]\n",
      "SubGD iter. 70/500: loss=25.81721232277017, w=[4.82792079e+01 3.30857411e-02]\n",
      "SubGD iter. 71/500: loss=25.15503943465645, w=[48.96534653  0.05502364]\n",
      "SubGD iter. 72/500: loss=24.524103413894778, w=[49.63069307  0.10538326]\n",
      "SubGD iter. 73/500: loss=23.899295346035593, w=[50.28910891  0.16746569]\n",
      "SubGD iter. 74/500: loss=23.284392925657144, w=[50.94752475  0.22954811]\n",
      "SubGD iter. 75/500: loss=22.686876444181845, w=[51.59207921  0.31242513]\n",
      "SubGD iter. 76/500: loss=22.10626756964055, w=[52.22277228  0.41195013]\n",
      "SubGD iter. 77/500: loss=21.537818828008433, w=[52.84653465  0.52081678]\n",
      "SubGD iter. 78/500: loss=20.986339874628463, w=[53.45643564  0.64579009]\n",
      "SubGD iter. 79/500: loss=20.445560936620446, w=[54.05940594  0.77969045]\n",
      "SubGD iter. 80/500: loss=19.91191015895785, w=[54.65544554  0.91975701]\n",
      "SubGD iter. 81/500: loss=19.389644090563234, w=[55.24455446  1.06709203]\n",
      "SubGD iter. 82/500: loss=18.887989064395885, w=[55.81980198  1.22612559]\n",
      "SubGD iter. 83/500: loss=18.415960501854236, w=[56.36732673  1.41070934]\n",
      "SubGD iter. 84/500: loss=17.954898543040386, w=[56.9009901   1.60585373]\n",
      "SubGD iter. 85/500: loss=17.505757656579824, w=[57.42772277  1.8087628 ]\n",
      "SubGD iter. 86/500: loss=17.07495742693161, w=[57.93366337  2.02850642]\n",
      "SubGD iter. 87/500: loss=16.652967297509903, w=[58.43267327  2.24943708]\n",
      "SubGD iter. 88/500: loss=16.24854073149673, w=[58.91089109  2.4837983 ]\n",
      "SubGD iter. 89/500: loss=15.849105212654159, w=[59.38217822  2.72602456]\n",
      "SubGD iter. 90/500: loss=15.46691979123133, w=[59.83960396  2.97874233]\n",
      "SubGD iter. 91/500: loss=15.108294621512215, w=[60.26237624  3.25152867]\n",
      "SubGD iter. 92/500: loss=14.754896345922832, w=[60.67821782  3.52708658]\n",
      "SubGD iter. 93/500: loss=14.40452896162028, w=[61.08712871  3.80645918]\n",
      "SubGD iter. 94/500: loss=14.055787028127279, w=[61.4960396   4.08583179]\n",
      "SubGD iter. 95/500: loss=13.714620911605635, w=[61.89108911  4.37383938]\n",
      "SubGD iter. 96/500: loss=13.381236307284155, w=[62.27920792  4.66603747]\n",
      "SubGD iter. 97/500: loss=13.058821615166238, w=[62.65346535  4.95982909]\n",
      "SubGD iter. 98/500: loss=12.74025172433924, w=[63.02079208  5.25705719]\n",
      "SubGD iter. 99/500: loss=12.42321888875611, w=[63.38118812  5.56043432]\n",
      "SubGD iter. 100/500: loss=12.107561731901173, w=[63.74158416  5.86381144]\n",
      "SubGD iter. 101/500: loss=11.800622097398135, w=[64.08811881  6.17240218]\n",
      "SubGD iter. 102/500: loss=11.495041794646427, w=[64.42772277  6.48636931]\n",
      "SubGD iter. 103/500: loss=11.189461491894715, w=[64.76732673  6.80033645]\n",
      "SubGD iter. 104/500: loss=10.883881189143004, w=[65.10693069  7.11430358]\n",
      "SubGD iter. 105/500: loss=10.584593408313202, w=[65.44653465  7.42827072]\n",
      "SubGD iter. 106/500: loss=10.295816534318941, w=[65.76534653  7.74789321]\n",
      "SubGD iter. 107/500: loss=10.01135208122136, w=[66.07029703  8.07366969]\n",
      "SubGD iter. 108/500: loss=9.72808432666813, w=[66.37524752  8.39944616]\n",
      "SubGD iter. 109/500: loss=9.44812546112251, w=[66.66633663  8.73297028]\n",
      "SubGD iter. 110/500: loss=9.17104110409667, w=[66.95742574  9.0664944 ]\n",
      "SubGD iter. 111/500: loss=8.903656131158963, w=[67.23465347  9.39863032]\n",
      "SubGD iter. 112/500: loss=8.636271158221255, w=[67.51188119  9.73076624]\n",
      "SubGD iter. 113/500: loss=8.376151920302375, w=[67.78910891 10.06290216]\n",
      "SubGD iter. 114/500: loss=8.140540838751498, w=[68.06633663 10.36399929]\n",
      "SubGD iter. 115/500: loss=7.918544501597273, w=[68.32970297 10.66046691]\n",
      "SubGD iter. 116/500: loss=7.705279728377, w=[68.59306931 10.94317438]\n",
      "SubGD iter. 117/500: loss=7.493695831178641, w=[68.85643564 11.22588185]\n",
      "SubGD iter. 118/500: loss=7.289992405743416, w=[69.11287129 11.50439584]\n",
      "SubGD iter. 119/500: loss=7.097234035781543, w=[69.35544554 11.78820189]\n",
      "SubGD iter. 120/500: loss=6.919905294668923, w=[69.58415842 12.06091147]\n",
      "SubGD iter. 121/500: loss=6.750573527315454, w=[69.80594059 12.32424567]\n",
      "SubGD iter. 122/500: loss=6.584744810805664, w=[70.02772277 12.58757987]\n",
      "SubGD iter. 123/500: loss=6.430343276347806, w=[70.25643564 12.82476541]\n",
      "SubGD iter. 124/500: loss=6.278071481890353, w=[70.47821782 13.06561696]\n",
      "SubGD iter. 125/500: loss=6.133663329263324, w=[70.69306931 13.30295339]\n",
      "SubGD iter. 126/500: loss=6.00584079834303, w=[70.89405941 13.5254031 ]\n",
      "SubGD iter. 127/500: loss=5.885021825223219, w=[71.08811881 13.74294562]\n",
      "SubGD iter. 128/500: loss=5.771635252269659, w=[71.27524752 13.9535482 ]\n",
      "SubGD iter. 129/500: loss=5.667162061790258, w=[71.46237624 14.16415077]\n",
      "SubGD iter. 130/500: loss=5.586726765993146, w=[71.62178218 14.34977956]\n",
      "SubGD iter. 131/500: loss=5.523847812160388, w=[71.75346535 14.51689011]\n",
      "SubGD iter. 132/500: loss=5.480093708591872, w=[71.87128713 14.67079119]\n",
      "SubGD iter. 133/500: loss=5.4530880035020255, w=[71.95445545 14.78027646]\n",
      "SubGD iter. 134/500: loss=5.427392630862905, w=[72.03762376 14.88976173]\n",
      "SubGD iter. 135/500: loss=5.407322445682752, w=[72.10693069 14.98591618]\n",
      "SubGD iter. 136/500: loss=5.387252260502599, w=[72.17623762 15.08207064]\n",
      "SubGD iter. 137/500: loss=5.3704607803386955, w=[72.24554455 15.17822509]\n",
      "SubGD iter. 138/500: loss=5.357406523334741, w=[72.3009901  15.25972349]\n",
      "SubGD iter. 139/500: loss=5.345929264022583, w=[72.34950495 15.33509186]\n",
      "SubGD iter. 140/500: loss=5.335714659517474, w=[72.3980198  15.41046022]\n",
      "SubGD iter. 141/500: loss=5.330043910465361, w=[72.43267327 15.46996179]\n",
      "SubGD iter. 142/500: loss=5.325676428273225, w=[72.46039604 15.51864529]\n",
      "SubGD iter. 143/500: loss=5.322176726526591, w=[72.48811881 15.56159216]\n",
      "SubGD iter. 144/500: loss=5.320111309643112, w=[72.5019802  15.59782833]\n",
      "SubGD iter. 145/500: loss=5.318478284898438, w=[72.52277228 15.62472286]\n",
      "SubGD iter. 146/500: loss=5.3172400485651465, w=[72.55049505 15.64269033]\n",
      "SubGD iter. 147/500: loss=5.316406547951546, w=[72.56435644 15.66435658]\n",
      "SubGD iter. 148/500: loss=5.315557122666144, w=[72.58514851 15.67709578]\n",
      "SubGD iter. 149/500: loss=5.314707697380741, w=[72.60594059 15.68983497]\n",
      "SubGD iter. 150/500: loss=5.313876880922167, w=[72.62673267 15.70257417]\n",
      "SubGD iter. 151/500: loss=5.313052246871384, w=[72.64059406 15.72424042]\n",
      "SubGD iter. 152/500: loss=5.312377839024387, w=[72.66138614 15.73697962]\n",
      "SubGD iter. 153/500: loss=5.312132229725043, w=[72.66831683 15.74811029]\n",
      "SubGD iter. 154/500: loss=5.311886620425697, w=[72.67524752 15.75924097]\n",
      "SubGD iter. 155/500: loss=5.311683566098434, w=[72.68217822 15.77037165]\n",
      "SubGD iter. 156/500: loss=5.311661251291322, w=[72.68217822 15.77432391]\n",
      "SubGD iter. 157/500: loss=5.31163893648421, w=[72.68217822 15.77827617]\n",
      "SubGD iter. 158/500: loss=5.311616621677096, w=[72.68217822 15.78222843]\n",
      "SubGD iter. 159/500: loss=5.311594306869984, w=[72.68217822 15.78618069]\n",
      "SubGD iter. 160/500: loss=5.311571992062871, w=[72.68217822 15.79013295]\n",
      "SubGD iter. 161/500: loss=5.31154967725576, w=[72.68217822 15.79408522]\n",
      "SubGD iter. 162/500: loss=5.311527362448647, w=[72.68217822 15.79803748]\n",
      "SubGD iter. 163/500: loss=5.311505047641535, w=[72.68217822 15.80198974]\n",
      "SubGD iter. 164/500: loss=5.311482732834422, w=[72.68217822 15.805942  ]\n",
      "SubGD iter. 165/500: loss=5.311460418027309, w=[72.68217822 15.80989426]\n",
      "SubGD iter. 166/500: loss=5.311438103220198, w=[72.68217822 15.81384652]\n",
      "SubGD iter. 167/500: loss=5.311415788413085, w=[72.68217822 15.81779878]\n",
      "SubGD iter. 168/500: loss=5.311393473605974, w=[72.68217822 15.82175104]\n",
      "SubGD iter. 169/500: loss=5.31137115879886, w=[72.68217822 15.8257033 ]\n",
      "SubGD iter. 170/500: loss=5.3113488439917464, w=[72.68217822 15.82965556]\n",
      "SubGD iter. 171/500: loss=5.311326529184636, w=[72.68217822 15.83360782]\n",
      "SubGD iter. 172/500: loss=5.311304214377523, w=[72.68217822 15.83756008]\n",
      "SubGD iter. 173/500: loss=5.311281899570409, w=[72.68217822 15.84151234]\n",
      "SubGD iter. 174/500: loss=5.311259584763298, w=[72.68217822 15.84546461]\n",
      "SubGD iter. 175/500: loss=5.311237269956186, w=[72.68217822 15.84941687]\n",
      "SubGD iter. 176/500: loss=5.311214955149072, w=[72.68217822 15.85336913]\n",
      "SubGD iter. 177/500: loss=5.31119264034196, w=[72.68217822 15.85732139]\n",
      "SubGD iter. 178/500: loss=5.311170325534848, w=[72.68217822 15.86127365]\n",
      "SubGD iter. 179/500: loss=5.311148010727736, w=[72.68217822 15.86522591]\n",
      "SubGD iter. 180/500: loss=5.311125695920623, w=[72.68217822 15.86917817]\n",
      "SubGD iter. 181/500: loss=5.31110338111351, w=[72.68217822 15.87313043]\n",
      "SubGD iter. 182/500: loss=5.311081066306398, w=[72.68217822 15.87708269]\n",
      "SubGD iter. 183/500: loss=5.311058751499285, w=[72.68217822 15.88103495]\n",
      "SubGD iter. 184/500: loss=5.3110364366921745, w=[72.68217822 15.88498721]\n",
      "SubGD iter. 185/500: loss=5.311014121885061, w=[72.68217822 15.88893947]\n",
      "SubGD iter. 186/500: loss=5.310991807077948, w=[72.68217822 15.89289173]\n",
      "SubGD iter. 187/500: loss=5.310969492270837, w=[72.68217822 15.89684399]\n",
      "SubGD iter. 188/500: loss=5.310947177463723, w=[72.68217822 15.90079626]\n",
      "SubGD iter. 189/500: loss=5.31092486265661, w=[72.68217822 15.90474852]\n",
      "SubGD iter. 190/500: loss=5.310902547849499, w=[72.68217822 15.90870078]\n",
      "SubGD iter. 191/500: loss=5.310913706061381, w=[72.68217822 15.91265304]\n",
      "SubGD iter. 192/500: loss=5.31089223718627, w=[72.67524752 15.91052694]\n",
      "SubGD iter. 193/500: loss=5.3108699223791564, w=[72.67524752 15.9144792 ]\n",
      "SubGD iter. 194/500: loss=5.310862636053835, w=[72.67524752 15.91843146]\n",
      "SubGD iter. 195/500: loss=5.310859611715927, w=[72.66831683 15.91630536]\n",
      "SubGD iter. 196/500: loss=5.310837296908816, w=[72.66831683 15.92025762]\n",
      "SubGD iter. 197/500: loss=5.310814982101703, w=[72.66831683 15.92420988]\n",
      "SubGD iter. 198/500: loss=5.310823570190169, w=[72.66831683 15.92816214]\n",
      "SubGD iter. 199/500: loss=5.310804671438475, w=[72.66138614 15.92603604]\n",
      "SubGD iter. 200/500: loss=5.3107823566313614, w=[72.66138614 15.9299883 ]\n",
      "SubGD iter. 201/500: loss=5.310772500182623, w=[72.66138614 15.93394056]\n",
      "SubGD iter. 202/500: loss=5.3107720459681325, w=[72.65445545 15.93181446]\n",
      "SubGD iter. 203/500: loss=5.310749731161019, w=[72.65445545 15.93576672]\n",
      "SubGD iter. 204/500: loss=5.310727416353908, w=[72.65445545 15.93971899]\n",
      "SubGD iter. 205/500: loss=5.310733434318959, w=[72.65445545 15.94367125]\n",
      "SubGD iter. 206/500: loss=5.310717105690678, w=[72.64752475 15.94154515]\n",
      "SubGD iter. 207/500: loss=5.3106947908835656, w=[72.64752475 15.94549741]\n",
      "SubGD iter. 208/500: loss=5.310682364311412, w=[72.64752475 15.94944967]\n",
      "SubGD iter. 209/500: loss=5.310684480220337, w=[72.64059406 15.94732357]\n",
      "SubGD iter. 210/500: loss=5.310662165413224, w=[72.64059406 15.95127583]\n",
      "SubGD iter. 211/500: loss=5.310639850606112, w=[72.64059406 15.95522809]\n",
      "SubGD iter. 212/500: loss=5.310643298447746, w=[72.64059406 15.95918035]\n",
      "SubGD iter. 213/500: loss=5.310629539942882, w=[72.63366337 15.95705425]\n",
      "SubGD iter. 214/500: loss=5.3106072251357705, w=[72.63366337 15.96100651]\n",
      "SubGD iter. 215/500: loss=5.310592228440201, w=[72.63366337 15.96495877]\n",
      "SubGD iter. 216/500: loss=5.310633183099028, w=[72.62673267 15.96283267]\n",
      "SubGD iter. 217/500: loss=5.310599343585063, w=[72.63366337 15.96730137]\n",
      "SubGD iter. 218/500: loss=5.31061822827579, w=[72.62673267 15.96517527]\n",
      "SubGD iter. 219/500: loss=5.310606458729926, w=[72.63366337 15.96964397]\n",
      "SubGD iter. 220/500: loss=5.3106032734525535, w=[72.62673267 15.96751787]\n",
      "SubGD iter. 221/500: loss=5.3106135738747895, w=[72.63366337 15.97198657]\n",
      "SubGD iter. 222/500: loss=5.310588318629316, w=[72.62673267 15.96986047]\n",
      "SubGD iter. 223/500: loss=5.310620689019651, w=[72.63366337 15.97432917]\n",
      "SubGD iter. 224/500: loss=5.310574966149887, w=[72.62673267 15.97220307]\n",
      "SubGD iter. 225/500: loss=5.310583639649728, w=[72.62673267 15.97059341]\n",
      "SubGD iter. 226/500: loss=5.310622915165495, w=[72.63366337 15.97506211]\n",
      "SubGD iter. 227/500: loss=5.310576651555033, w=[72.62673267 15.97293601]\n",
      "SubGD iter. 228/500: loss=5.31057896067014, w=[72.62673267 15.97132635]\n",
      "SubGD iter. 229/500: loss=5.310625141311338, w=[72.63366337 15.97579505]\n",
      "SubGD iter. 230/500: loss=5.310578336960181, w=[72.62673267 15.97366895]\n",
      "SubGD iter. 231/500: loss=5.310574635520699, w=[72.62673267 15.97205929]\n",
      "SubGD iter. 232/500: loss=5.310584557534204, w=[72.62673267 15.97044963]\n",
      "SubGD iter. 233/500: loss=5.31062247845816, w=[72.63366337 15.97491833]\n",
      "SubGD iter. 234/500: loss=5.310576320925847, w=[72.62673267 15.97279223]\n",
      "SubGD iter. 235/500: loss=5.3105798785546146, w=[72.62673267 15.97118257]\n",
      "SubGD iter. 236/500: loss=5.310624704604003, w=[72.63366337 15.97565127]\n",
      "SubGD iter. 237/500: loss=5.310578006330994, w=[72.62673267 15.97352517]\n",
      "SubGD iter. 238/500: loss=5.310575199575027, w=[72.62673267 15.97191551]\n",
      "SubGD iter. 239/500: loss=5.310626930749845, w=[72.63366337 15.97638421]\n",
      "SubGD iter. 240/500: loss=5.310579691736141, w=[72.62673267 15.97425811]\n",
      "SubGD iter. 241/500: loss=5.310575990296659, w=[72.62673267 15.97264845]\n",
      "SubGD iter. 242/500: loss=5.310580796439089, w=[72.62673267 15.97103879]\n",
      "SubGD iter. 243/500: loss=5.310624267896667, w=[72.63366337 15.97550749]\n",
      "SubGD iter. 244/500: loss=5.310577675701806, w=[72.62673267 15.97338139]\n",
      "SubGD iter. 245/500: loss=5.310576117459501, w=[72.62673267 15.97177173]\n",
      "SubGD iter. 246/500: loss=5.31062649404251, w=[72.63366337 15.97624043]\n",
      "SubGD iter. 247/500: loss=5.310579361106954, w=[72.62673267 15.97411433]\n",
      "SubGD iter. 248/500: loss=5.310575659667472, w=[72.62673267 15.97250467]\n",
      "SubGD iter. 249/500: loss=5.310581714323563, w=[72.62673267 15.970895  ]\n",
      "SubGD iter. 250/500: loss=5.310623831189332, w=[72.63366337 15.9753637 ]\n",
      "SubGD iter. 251/500: loss=5.31057734507262, w=[72.62673267 15.9732376 ]\n",
      "SubGD iter. 252/500: loss=5.310577035343975, w=[72.62673267 15.97162794]\n",
      "SubGD iter. 253/500: loss=5.310626057335176, w=[72.63366337 15.97609664]\n",
      "SubGD iter. 254/500: loss=5.310579030477766, w=[72.62673267 15.97397054]\n",
      "SubGD iter. 255/500: loss=5.3105753290382856, w=[72.62673267 15.97236088]\n",
      "SubGD iter. 256/500: loss=5.310582632208036, w=[72.62673267 15.97075122]\n",
      "SubGD iter. 257/500: loss=5.310623394481998, w=[72.63366337 15.97521992]\n",
      "SubGD iter. 258/500: loss=5.3105770144434326, w=[72.62673267 15.97309382]\n",
      "SubGD iter. 259/500: loss=5.3105779532284485, w=[72.62673267 15.97148416]\n",
      "SubGD iter. 260/500: loss=5.3106256206278415, w=[72.63366337 15.97595286]\n",
      "SubGD iter. 261/500: loss=5.31057869984858, w=[72.62673267 15.97382676]\n",
      "SubGD iter. 262/500: loss=5.310574998409098, w=[72.62673267 15.9722171 ]\n",
      "SubGD iter. 263/500: loss=5.3105835500925105, w=[72.62673267 15.97060744]\n",
      "SubGD iter. 264/500: loss=5.3106229577746635, w=[72.63366337 15.97507614]\n",
      "SubGD iter. 265/500: loss=5.310576683814246, w=[72.62673267 15.97295004]\n",
      "SubGD iter. 266/500: loss=5.310578871112924, w=[72.62673267 15.97134038]\n",
      "SubGD iter. 267/500: loss=5.310625183920507, w=[72.63366337 15.97580908]\n",
      "SubGD iter. 268/500: loss=5.310578369219393, w=[72.62673267 15.97368298]\n",
      "SubGD iter. 269/500: loss=5.310574667779911, w=[72.62673267 15.97207332]\n",
      "SubGD iter. 270/500: loss=5.310584467976984, w=[72.62673267 15.97046366]\n",
      "SubGD iter. 271/500: loss=5.310622521067329, w=[72.63366337 15.97493236]\n",
      "SubGD iter. 272/500: loss=5.310576353185058, w=[72.62673267 15.97280626]\n",
      "SubGD iter. 273/500: loss=5.310579788997396, w=[72.62673267 15.9711966 ]\n",
      "SubGD iter. 274/500: loss=5.310624747213171, w=[72.63366337 15.9756653 ]\n",
      "SubGD iter. 275/500: loss=5.310578038590206, w=[72.62673267 15.9735392 ]\n",
      "SubGD iter. 276/500: loss=5.310575110017808, w=[72.62673267 15.97192954]\n",
      "SubGD iter. 277/500: loss=5.310626973359014, w=[72.63366337 15.97639824]\n",
      "SubGD iter. 278/500: loss=5.310579723995353, w=[72.62673267 15.97427214]\n",
      "SubGD iter. 279/500: loss=5.310576022555872, w=[72.62673267 15.97266248]\n",
      "SubGD iter. 280/500: loss=5.31058070688187, w=[72.62673267 15.97105282]\n",
      "SubGD iter. 281/500: loss=5.310624310505837, w=[72.63366337 15.97552152]\n",
      "SubGD iter. 282/500: loss=5.310577707961019, w=[72.62673267 15.97339542]\n",
      "SubGD iter. 283/500: loss=5.310576027902282, w=[72.62673267 15.97178575]\n",
      "SubGD iter. 284/500: loss=5.31062653665168, w=[72.63366337 15.97625445]\n",
      "SubGD iter. 285/500: loss=5.310579393366166, w=[72.62673267 15.97412835]\n",
      "SubGD iter. 286/500: loss=5.310575691926684, w=[72.62673267 15.97251869]\n",
      "SubGD iter. 287/500: loss=5.3105816247663435, w=[72.62673267 15.97090903]\n",
      "SubGD iter. 288/500: loss=5.310623873798502, w=[72.63366337 15.97537773]\n",
      "SubGD iter. 289/500: loss=5.310577377331832, w=[72.62673267 15.97325163]\n",
      "SubGD iter. 290/500: loss=5.310576945786757, w=[72.62673267 15.97164197]\n",
      "SubGD iter. 291/500: loss=5.3106260999443435, w=[72.63366337 15.97611067]\n",
      "SubGD iter. 292/500: loss=5.310579062736979, w=[72.62673267 15.97398457]\n",
      "SubGD iter. 293/500: loss=5.310575361297499, w=[72.62673267 15.97237491]\n",
      "SubGD iter. 294/500: loss=5.310582542650818, w=[72.62673267 15.97076525]\n",
      "SubGD iter. 295/500: loss=5.310623437091167, w=[72.63366337 15.97523395]\n",
      "SubGD iter. 296/500: loss=5.310577046702645, w=[72.62673267 15.97310785]\n",
      "SubGD iter. 297/500: loss=5.31057786367123, w=[72.62673267 15.97149819]\n",
      "SubGD iter. 298/500: loss=5.310625663237009, w=[72.63366337 15.97596689]\n",
      "SubGD iter. 299/500: loss=5.310578732107792, w=[72.62673267 15.97384079]\n",
      "SubGD iter. 300/500: loss=5.310575030668311, w=[72.62673267 15.97223113]\n",
      "SubGD iter. 301/500: loss=5.31058346053529, w=[72.62673267 15.97062147]\n",
      "SubGD iter. 302/500: loss=5.310623000383831, w=[72.63366337 15.97509017]\n",
      "SubGD iter. 303/500: loss=5.3105767160734585, w=[72.62673267 15.97296407]\n",
      "SubGD iter. 304/500: loss=5.310578781555702, w=[72.62673267 15.97135441]\n",
      "SubGD iter. 305/500: loss=5.310625226529674, w=[72.63366337 15.97582311]\n",
      "SubGD iter. 306/500: loss=5.3105784014786055, w=[72.62673267 15.97369701]\n",
      "SubGD iter. 307/500: loss=5.3105747000391235, w=[72.62673267 15.97208735]\n",
      "SubGD iter. 308/500: loss=5.310584378419766, w=[72.62673267 15.97047769]\n",
      "SubGD iter. 309/500: loss=5.310622563676496, w=[72.63366337 15.97494639]\n",
      "SubGD iter. 310/500: loss=5.3105763854442705, w=[72.62673267 15.97282029]\n",
      "SubGD iter. 311/500: loss=5.310579699440178, w=[72.62673267 15.97121063]\n",
      "SubGD iter. 312/500: loss=5.31062478982234, w=[72.63366337 15.97567933]\n",
      "SubGD iter. 313/500: loss=5.310578070849419, w=[72.62673267 15.97355323]\n",
      "SubGD iter. 314/500: loss=5.310575020460591, w=[72.62673267 15.97194357]\n",
      "SubGD iter. 315/500: loss=5.3106270159681825, w=[72.63366337 15.97641227]\n",
      "SubGD iter. 316/500: loss=5.310579756254565, w=[72.62673267 15.97428617]\n",
      "SubGD iter. 317/500: loss=5.310576054815085, w=[72.62673267 15.9726765 ]\n",
      "SubGD iter. 318/500: loss=5.310580617324651, w=[72.62673267 15.97106684]\n",
      "SubGD iter. 319/500: loss=5.3106243531150055, w=[72.63366337 15.97553554]\n",
      "SubGD iter. 320/500: loss=5.310577740220231, w=[72.62673267 15.97340944]\n",
      "SubGD iter. 321/500: loss=5.310575938345063, w=[72.62673267 15.97179978]\n",
      "SubGD iter. 322/500: loss=5.310626579260847, w=[72.63366337 15.97626848]\n",
      "SubGD iter. 323/500: loss=5.310579425625379, w=[72.62673267 15.97414238]\n",
      "SubGD iter. 324/500: loss=5.310575724185897, w=[72.62673267 15.97253272]\n",
      "SubGD iter. 325/500: loss=5.310581535209124, w=[72.62673267 15.97092306]\n",
      "SubGD iter. 326/500: loss=5.310623916407671, w=[72.63366337 15.97539176]\n",
      "SubGD iter. 327/500: loss=5.310577409591045, w=[72.62673267 15.97326566]\n",
      "SubGD iter. 328/500: loss=5.310576856229538, w=[72.62673267 15.971656  ]\n",
      "SubGD iter. 329/500: loss=5.310626142553512, w=[72.63366337 15.9761247 ]\n",
      "SubGD iter. 330/500: loss=5.310579094996192, w=[72.62673267 15.9739986 ]\n",
      "SubGD iter. 331/500: loss=5.31057539355671, w=[72.62673267 15.97238894]\n",
      "SubGD iter. 332/500: loss=5.310582453093599, w=[72.62673267 15.97077928]\n",
      "SubGD iter. 333/500: loss=5.310623479700335, w=[72.63366337 15.97524798]\n",
      "SubGD iter. 334/500: loss=5.310577078961859, w=[72.62673267 15.97312188]\n",
      "SubGD iter. 335/500: loss=5.3105777741140106, w=[72.62673267 15.97151222]\n",
      "SubGD iter. 336/500: loss=5.310625705846178, w=[72.63366337 15.97598092]\n",
      "SubGD iter. 337/500: loss=5.310578764367005, w=[72.62673267 15.97385482]\n",
      "SubGD iter. 338/500: loss=5.3105750629275255, w=[72.62673267 15.97224516]\n",
      "SubGD iter. 339/500: loss=5.3105833709780725, w=[72.62673267 15.9706355 ]\n",
      "SubGD iter. 340/500: loss=5.310623042993001, w=[72.63366337 15.9751042 ]\n",
      "SubGD iter. 341/500: loss=5.310576748332672, w=[72.62673267 15.9729781 ]\n",
      "SubGD iter. 342/500: loss=5.310578691998486, w=[72.62673267 15.97136844]\n",
      "SubGD iter. 343/500: loss=5.310625269138844, w=[72.63366337 15.97583714]\n",
      "SubGD iter. 344/500: loss=5.310578433737818, w=[72.62673267 15.97371104]\n",
      "SubGD iter. 345/500: loss=5.3105747322983365, w=[72.62673267 15.97210138]\n",
      "SubGD iter. 346/500: loss=5.310584288862546, w=[72.62673267 15.97049172]\n",
      "SubGD iter. 347/500: loss=5.3106226062856665, w=[72.63366337 15.97496042]\n",
      "SubGD iter. 348/500: loss=5.310576417703483, w=[72.62673267 15.97283432]\n",
      "SubGD iter. 349/500: loss=5.310579609882957, w=[72.62673267 15.97122465]\n",
      "SubGD iter. 350/500: loss=5.310624832431509, w=[72.63366337 15.97569335]\n",
      "SubGD iter. 351/500: loss=5.3105781031086305, w=[72.62673267 15.97356725]\n",
      "SubGD iter. 352/500: loss=5.310574930903371, w=[72.62673267 15.97195759]\n",
      "SubGD iter. 353/500: loss=5.310627058577351, w=[72.63366337 15.97642629]\n",
      "SubGD iter. 354/500: loss=5.3105797885137775, w=[72.62673267 15.97430019]\n",
      "SubGD iter. 355/500: loss=5.310576087074297, w=[72.62673267 15.97269053]\n",
      "SubGD iter. 356/500: loss=5.310580527767432, w=[72.62673267 15.97108087]\n",
      "SubGD iter. 357/500: loss=5.310624395724175, w=[72.63366337 15.97554957]\n",
      "SubGD iter. 358/500: loss=5.310577772479444, w=[72.62673267 15.97342347]\n",
      "SubGD iter. 359/500: loss=5.3105758487878445, w=[72.62673267 15.97181381]\n",
      "SubGD iter. 360/500: loss=5.310626621870016, w=[72.63366337 15.97628251]\n",
      "SubGD iter. 361/500: loss=5.310579457884592, w=[72.62673267 15.97415641]\n",
      "SubGD iter. 362/500: loss=5.31057575644511, w=[72.62673267 15.97254675]\n",
      "SubGD iter. 363/500: loss=5.310581445651906, w=[72.62673267 15.97093709]\n",
      "SubGD iter. 364/500: loss=5.310623959016839, w=[72.63366337 15.97540579]\n",
      "SubGD iter. 365/500: loss=5.310577441850257, w=[72.62673267 15.97327969]\n",
      "SubGD iter. 366/500: loss=5.310576766672319, w=[72.62673267 15.97167003]\n",
      "SubGD iter. 367/500: loss=5.310626185162682, w=[72.63366337 15.97613873]\n",
      "SubGD iter. 368/500: loss=5.310579127255404, w=[72.62673267 15.97401263]\n",
      "SubGD iter. 369/500: loss=5.310575425815924, w=[72.62673267 15.97240297]\n",
      "SubGD iter. 370/500: loss=5.31058236353638, w=[72.62673267 15.97079331]\n",
      "SubGD iter. 371/500: loss=5.310623522309504, w=[72.63366337 15.97526201]\n",
      "SubGD iter. 372/500: loss=5.31057711122107, w=[72.62673267 15.97313591]\n",
      "SubGD iter. 373/500: loss=5.310577684556793, w=[72.62673267 15.97152625]\n",
      "SubGD iter. 374/500: loss=5.3106257484553465, w=[72.63366337 15.97599495]\n",
      "SubGD iter. 375/500: loss=5.310578796626218, w=[72.62673267 15.97386885]\n",
      "SubGD iter. 376/500: loss=5.310575095186736, w=[72.62673267 15.97225919]\n",
      "SubGD iter. 377/500: loss=5.310583281420854, w=[72.62673267 15.97064953]\n",
      "SubGD iter. 378/500: loss=5.3106230856021694, w=[72.63366337 15.97511823]\n",
      "SubGD iter. 379/500: loss=5.310576780591885, w=[72.62673267 15.97299213]\n",
      "SubGD iter. 380/500: loss=5.310578602441266, w=[72.62673267 15.97138247]\n",
      "SubGD iter. 381/500: loss=5.310625311748012, w=[72.63366337 15.97585117]\n",
      "SubGD iter. 382/500: loss=5.310578465997031, w=[72.62673267 15.97372507]\n",
      "SubGD iter. 383/500: loss=5.31057476455755, w=[72.62673267 15.97211541]\n",
      "SubGD iter. 384/500: loss=5.310584199305326, w=[72.62673267 15.97050574]\n",
      "SubGD iter. 385/500: loss=5.310622648894835, w=[72.63366337 15.97497444]\n",
      "SubGD iter. 386/500: loss=5.310576449962697, w=[72.62673267 15.97284834]\n",
      "SubGD iter. 387/500: loss=5.310579520325739, w=[72.62673267 15.97123868]\n",
      "SubGD iter. 388/500: loss=5.310624875040677, w=[72.63366337 15.97570738]\n",
      "SubGD iter. 389/500: loss=5.3105781353678445, w=[72.62673267 15.97358128]\n",
      "SubGD iter. 390/500: loss=5.310574841346153, w=[72.62673267 15.97197162]\n",
      "SubGD iter. 391/500: loss=5.31062710118652, w=[72.63366337 15.97644032]\n",
      "SubGD iter. 392/500: loss=5.3105798207729915, w=[72.62673267 15.97431422]\n",
      "SubGD iter. 393/500: loss=5.3105761193335095, w=[72.62673267 15.97270456]\n",
      "SubGD iter. 394/500: loss=5.310580438210215, w=[72.62673267 15.9710949 ]\n",
      "SubGD iter. 395/500: loss=5.310624438333342, w=[72.63366337 15.9755636 ]\n",
      "SubGD iter. 396/500: loss=5.3105778047386565, w=[72.62673267 15.9734375 ]\n",
      "SubGD iter. 397/500: loss=5.310575759230627, w=[72.62673267 15.97182784]\n",
      "SubGD iter. 398/500: loss=5.3106266644791855, w=[72.63366337 15.97629654]\n",
      "SubGD iter. 399/500: loss=5.3105794901438035, w=[72.62673267 15.97417044]\n",
      "SubGD iter. 400/500: loss=5.310575788704323, w=[72.62673267 15.97256078]\n",
      "SubGD iter. 401/500: loss=5.310581356094687, w=[72.62673267 15.97095112]\n",
      "SubGD iter. 402/500: loss=5.310624001626008, w=[72.63366337 15.97541982]\n",
      "SubGD iter. 403/500: loss=5.31057747410947, w=[72.62673267 15.97329372]\n",
      "SubGD iter. 404/500: loss=5.310576677115099, w=[72.62673267 15.97168406]\n",
      "SubGD iter. 405/500: loss=5.31062622777185, w=[72.63366337 15.97615276]\n",
      "SubGD iter. 406/500: loss=5.310579159514617, w=[72.62673267 15.97402666]\n",
      "SubGD iter. 407/500: loss=5.310575458075137, w=[72.62673267 15.972417  ]\n",
      "SubGD iter. 408/500: loss=5.310582273979161, w=[72.62673267 15.97080734]\n",
      "SubGD iter. 409/500: loss=5.310623564918673, w=[72.63366337 15.97527604]\n",
      "SubGD iter. 410/500: loss=5.310577143480284, w=[72.62673267 15.97314994]\n",
      "SubGD iter. 411/500: loss=5.310577594999573, w=[72.62673267 15.97154028]\n",
      "SubGD iter. 412/500: loss=5.310625791064515, w=[72.63366337 15.97600898]\n",
      "SubGD iter. 413/500: loss=5.31057882888543, w=[72.62673267 15.97388288]\n",
      "SubGD iter. 414/500: loss=5.310575127445949, w=[72.62673267 15.97227322]\n",
      "SubGD iter. 415/500: loss=5.3105831918636355, w=[72.62673267 15.97066356]\n",
      "SubGD iter. 416/500: loss=5.310623128211338, w=[72.63366337 15.97513225]\n",
      "SubGD iter. 417/500: loss=5.310576812851096, w=[72.62673267 15.97300616]\n",
      "SubGD iter. 418/500: loss=5.310578512884048, w=[72.62673267 15.97139649]\n",
      "SubGD iter. 419/500: loss=5.31062535435718, w=[72.63366337 15.97586519]\n",
      "SubGD iter. 420/500: loss=5.310578498256244, w=[72.62673267 15.97373909]\n",
      "SubGD iter. 421/500: loss=5.310574796816762, w=[72.62673267 15.97212943]\n",
      "SubGD iter. 422/500: loss=5.31058410974811, w=[72.62673267 15.97051977]\n",
      "SubGD iter. 423/500: loss=5.310622691504003, w=[72.63366337 15.97498847]\n",
      "SubGD iter. 424/500: loss=5.310576482221909, w=[72.62673267 15.97286237]\n",
      "SubGD iter. 425/500: loss=5.310579430768521, w=[72.62673267 15.97125271]\n",
      "SubGD iter. 426/500: loss=5.310624917649847, w=[72.63366337 15.97572141]\n",
      "SubGD iter. 427/500: loss=5.310578167627056, w=[72.62673267 15.97359531]\n",
      "SubGD iter. 428/500: loss=5.310574751788934, w=[72.62673267 15.97198565]\n",
      "SubGD iter. 429/500: loss=5.310627143795689, w=[72.63366337 15.97645435]\n",
      "SubGD iter. 430/500: loss=5.310579853032204, w=[72.62673267 15.97432825]\n",
      "SubGD iter. 431/500: loss=5.310576151592722, w=[72.62673267 15.97271859]\n",
      "SubGD iter. 432/500: loss=5.310580348652994, w=[72.62673267 15.97110893]\n",
      "SubGD iter. 433/500: loss=5.310624480942511, w=[72.63366337 15.97557763]\n",
      "SubGD iter. 434/500: loss=5.31057783699787, w=[72.62673267 15.97345153]\n",
      "SubGD iter. 435/500: loss=5.310575669673408, w=[72.62673267 15.97184187]\n",
      "SubGD iter. 436/500: loss=5.310626707088354, w=[72.63366337 15.97631057]\n",
      "SubGD iter. 437/500: loss=5.3105795224030174, w=[72.62673267 15.97418447]\n",
      "SubGD iter. 438/500: loss=5.310575820963536, w=[72.62673267 15.97257481]\n",
      "SubGD iter. 439/500: loss=5.3105812665374685, w=[72.62673267 15.97096515]\n",
      "SubGD iter. 440/500: loss=5.310624044235177, w=[72.63366337 15.97543385]\n",
      "SubGD iter. 441/500: loss=5.310577506368683, w=[72.62673267 15.97330775]\n",
      "SubGD iter. 442/500: loss=5.310576587557881, w=[72.62673267 15.97169809]\n",
      "SubGD iter. 443/500: loss=5.310626270381019, w=[72.63366337 15.97616679]\n",
      "SubGD iter. 444/500: loss=5.31057919177383, w=[72.62673267 15.97404069]\n",
      "SubGD iter. 445/500: loss=5.310575490334348, w=[72.62673267 15.97243103]\n",
      "SubGD iter. 446/500: loss=5.310582184421943, w=[72.62673267 15.97082137]\n",
      "SubGD iter. 447/500: loss=5.310623607527843, w=[72.63366337 15.97529007]\n",
      "SubGD iter. 448/500: loss=5.310577175739496, w=[72.62673267 15.97316397]\n",
      "SubGD iter. 449/500: loss=5.310577505442355, w=[72.62673267 15.97155431]\n",
      "SubGD iter. 450/500: loss=5.310625833673684, w=[72.63366337 15.97602301]\n",
      "SubGD iter. 451/500: loss=5.310578861144643, w=[72.62673267 15.97389691]\n",
      "SubGD iter. 452/500: loss=5.310575159705161, w=[72.62673267 15.97228724]\n",
      "SubGD iter. 453/500: loss=5.310583102306416, w=[72.62673267 15.97067758]\n",
      "SubGD iter. 454/500: loss=5.310623170820506, w=[72.63366337 15.97514628]\n",
      "SubGD iter. 455/500: loss=5.310576845110308, w=[72.62673267 15.97302018]\n",
      "SubGD iter. 456/500: loss=5.310578423326828, w=[72.62673267 15.97141052]\n",
      "SubGD iter. 457/500: loss=5.3106253969663495, w=[72.63366337 15.97587922]\n",
      "SubGD iter. 458/500: loss=5.310578530515456, w=[72.62673267 15.97375312]\n",
      "SubGD iter. 459/500: loss=5.310574829075976, w=[72.62673267 15.97214346]\n",
      "SubGD iter. 460/500: loss=5.31058402019089, w=[72.62673267 15.9705338 ]\n",
      "SubGD iter. 461/500: loss=5.3106227341131715, w=[72.63366337 15.9750025 ]\n",
      "SubGD iter. 462/500: loss=5.310576514481123, w=[72.62673267 15.9728764 ]\n",
      "SubGD iter. 463/500: loss=5.3105793412113025, w=[72.62673267 15.97126674]\n",
      "SubGD iter. 464/500: loss=5.310624960259015, w=[72.63366337 15.97573544]\n",
      "SubGD iter. 465/500: loss=5.310578199886269, w=[72.62673267 15.97360934]\n",
      "SubGD iter. 466/500: loss=5.310574662231713, w=[72.62673267 15.97199968]\n",
      "SubGD iter. 467/500: loss=5.310627186404858, w=[72.63366337 15.97646838]\n",
      "SubGD iter. 468/500: loss=5.310579885291418, w=[72.62673267 15.97434228]\n",
      "SubGD iter. 469/500: loss=5.310576183851936, w=[72.62673267 15.97273262]\n",
      "SubGD iter. 470/500: loss=5.310580259095775, w=[72.62673267 15.97112296]\n",
      "SubGD iter. 471/500: loss=5.31062452355168, w=[72.63366337 15.97559166]\n",
      "SubGD iter. 472/500: loss=5.310577869257083, w=[72.62673267 15.97346556]\n",
      "SubGD iter. 473/500: loss=5.310575580116187, w=[72.62673267 15.9718559 ]\n",
      "SubGD iter. 474/500: loss=5.310626749697522, w=[72.63366337 15.9763246 ]\n",
      "SubGD iter. 475/500: loss=5.3105795546622305, w=[72.62673267 15.9741985 ]\n",
      "SubGD iter. 476/500: loss=5.3105758532227485, w=[72.62673267 15.97258884]\n",
      "SubGD iter. 477/500: loss=5.31058117698025, w=[72.62673267 15.97097918]\n",
      "SubGD iter. 478/500: loss=5.310624086844345, w=[72.63366337 15.97544788]\n",
      "SubGD iter. 479/500: loss=5.3105775386278955, w=[72.62673267 15.97332178]\n",
      "SubGD iter. 480/500: loss=5.3105764980006605, w=[72.62673267 15.97171212]\n",
      "SubGD iter. 481/500: loss=5.310626312990188, w=[72.63366337 15.97618082]\n",
      "SubGD iter. 482/500: loss=5.3105792240330425, w=[72.62673267 15.97405472]\n",
      "SubGD iter. 483/500: loss=5.3105755225935605, w=[72.62673267 15.97244506]\n",
      "SubGD iter. 484/500: loss=5.310582094864723, w=[72.62673267 15.97083539]\n",
      "SubGD iter. 485/500: loss=5.31062365013701, w=[72.63366337 15.97530409]\n",
      "SubGD iter. 486/500: loss=5.3105772079987075, w=[72.62673267 15.97317799]\n",
      "SubGD iter. 487/500: loss=5.3105774158851355, w=[72.62673267 15.97156833]\n",
      "SubGD iter. 488/500: loss=5.310625876282852, w=[72.63366337 15.97603703]\n",
      "SubGD iter. 489/500: loss=5.310578893403856, w=[72.62673267 15.97391093]\n",
      "SubGD iter. 490/500: loss=5.310575191964374, w=[72.62673267 15.97230127]\n",
      "SubGD iter. 491/500: loss=5.3105830127491975, w=[72.62673267 15.97069161]\n",
      "SubGD iter. 492/500: loss=5.310623213429675, w=[72.63366337 15.97516031]\n",
      "SubGD iter. 493/500: loss=5.310576877369521, w=[72.62673267 15.97303421]\n",
      "SubGD iter. 494/500: loss=5.31057833376961, w=[72.62673267 15.97142455]\n",
      "SubGD iter. 495/500: loss=5.310625439575518, w=[72.63366337 15.97589325]\n",
      "SubGD iter. 496/500: loss=5.310578562774669, w=[72.62673267 15.97376715]\n",
      "SubGD iter. 497/500: loss=5.310574861335188, w=[72.62673267 15.97215749]\n",
      "SubGD iter. 498/500: loss=5.310583930633671, w=[72.62673267 15.97054783]\n",
      "SubGD iter. 499/500: loss=5.310622776722341, w=[72.63366337 15.97501653]\n",
      "SubGD iter. 500/500: loss=5.310576546740335, w=[72.62673267 15.97289043]\n",
      "SubGD: execution time=0.022 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5025c5ed5ae4aacbb9842ec99001f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Compute a stochastic subgradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters+1 containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = np.zeros((max_iters+1,len(initial_w)))\n",
    "    ws[0] = initial_w\n",
    "    losses = np.zeros(max_iters)\n",
    "\n",
    "    for n in range(max_iters):\n",
    "        losses[n] = compute_loss(y,tx,ws[n],MAE)\n",
    "\n",
    "        batch_y, batch_x = mini_batch(y,tx,batch_size)\n",
    "\n",
    "        ws[n+1] = ws[n] - gamma * compute_subgradient_mae(batch_y,batch_x,ws[n])\n",
    "        print(f\"SubSGD iter. {n+1}/{max_iters}: loss={losses[n]}, w={ws[n]}\")\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 1/500: loss=74.06780585492638, w=[0. 0.]\n",
      "SubSGD iter. 2/500: loss=73.36780585492637, w=[ 0.7        -0.43968841]\n",
      "SubSGD iter. 3/500: loss=72.66780585492639, w=[ 1.4       -1.0645045]\n",
      "SubSGD iter. 4/500: loss=71.96780585492638, w=[2.1        0.05380572]\n",
      "SubSGD iter. 5/500: loss=71.26780585492638, w=[ 2.8        -0.76485265]\n",
      "SubSGD iter. 6/500: loss=70.56780585492638, w=[ 3.5       -0.8918671]\n",
      "SubSGD iter. 7/500: loss=69.86780585492637, w=[4.2        0.31414919]\n",
      "SubSGD iter. 8/500: loss=69.16780585492639, w=[ 4.9        -0.21280785]\n",
      "SubSGD iter. 9/500: loss=68.46780585492638, w=[5.6       0.2271166]\n",
      "SubSGD iter. 10/500: loss=67.76780585492638, w=[ 6.3        -0.46946815]\n",
      "SubSGD iter. 11/500: loss=67.06780585492638, w=[ 7.         -0.56974525]\n",
      "SubSGD iter. 12/500: loss=66.36780585492637, w=[ 7.7        -0.04168944]\n",
      "SubSGD iter. 13/500: loss=65.66780585492639, w=[ 8.4        -0.72228862]\n",
      "SubSGD iter. 14/500: loss=64.96780585492638, w=[ 9.1        -1.24947725]\n",
      "SubSGD iter. 15/500: loss=64.26780585492638, w=[ 9.8        -1.79249372]\n",
      "SubSGD iter. 16/500: loss=63.567805854926384, w=[10.5        -0.67418349]\n",
      "SubSGD iter. 17/500: loss=62.867805854926374, w=[11.2        -1.04010423]\n",
      "SubSGD iter. 18/500: loss=62.167805854926385, w=[11.9        -1.25736123]\n",
      "SubSGD iter. 19/500: loss=61.46780585492638, w=[12.6        -0.53919791]\n",
      "SubSGD iter. 20/500: loss=60.76780585492637, w=[ 1.33000000e+01 -9.49693138e-03]\n",
      "SubSGD iter. 21/500: loss=60.067805854926384, w=[14.         -0.79372966]\n",
      "SubSGD iter. 22/500: loss=59.36780585492639, w=[14.7        -0.56460797]\n",
      "SubSGD iter. 23/500: loss=58.667805854926385, w=[15.4         0.04930645]\n",
      "SubSGD iter. 24/500: loss=57.96780585492639, w=[16.1        -0.63738819]\n",
      "SubSGD iter. 25/500: loss=57.26780585492638, w=[16.8         0.48092203]\n",
      "SubSGD iter. 26/500: loss=56.567805854926384, w=[17.5        -0.54601017]\n",
      "SubSGD iter. 27/500: loss=55.867805854926374, w=[18.2        -1.38742868]\n",
      "SubSGD iter. 28/500: loss=55.1678058549264, w=[18.9        -2.00656983]\n",
      "SubSGD iter. 29/500: loss=54.4678058549264, w=[19.6        -2.22382684]\n",
      "SubSGD iter. 30/500: loss=53.76780585492638, w=[20.3        -3.20062431]\n",
      "SubSGD iter. 31/500: loss=53.067805854926405, w=[21.         -3.93473864]\n",
      "SubSGD iter. 32/500: loss=52.367805854926395, w=[21.7        -3.40503766]\n",
      "SubSGD iter. 33/500: loss=51.6678058549264, w=[22.4        -4.02985376]\n",
      "SubSGD iter. 34/500: loss=50.9678058549264, w=[23.1        -3.28386811]\n",
      "SubSGD iter. 35/500: loss=50.267805854926394, w=[23.8        -2.73547361]\n",
      "SubSGD iter. 36/500: loss=49.567805854926405, w=[24.5        -2.91893938]\n",
      "SubSGD iter. 37/500: loss=48.867805854926395, w=[25.2        -3.52270545]\n",
      "SubSGD iter. 38/500: loss=48.1678058549264, w=[25.9        -3.51627757]\n",
      "SubSGD iter. 39/500: loss=47.4678058549264, w=[26.6        -3.40245725]\n",
      "SubSGD iter. 40/500: loss=46.7678058549264, w=[27.3        -3.75918277]\n",
      "SubSGD iter. 41/500: loss=46.067805854926405, w=[28.         -3.85945986]\n",
      "SubSGD iter. 42/500: loss=45.3678058549264, w=[28.7        -4.25891764]\n",
      "SubSGD iter. 43/500: loss=44.66780585492641, w=[29.4        -4.50294446]\n",
      "SubSGD iter. 44/500: loss=43.9678058549264, w=[30.1        -3.66960688]\n",
      "SubSGD iter. 45/500: loss=43.2678058549264, w=[30.8        -3.79662132]\n",
      "SubSGD iter. 46/500: loss=42.567805854926405, w=[31.5        -4.69825359]\n",
      "SubSGD iter. 47/500: loss=41.8678058549264, w=[32.2       -4.9392335]\n",
      "SubSGD iter. 48/500: loss=41.16780585492641, w=[32.9        -5.71005657]\n",
      "SubSGD iter. 49/500: loss=40.4678058549264, w=[33.6        -6.73698877]\n",
      "SubSGD iter. 50/500: loss=39.767805854926394, w=[34.3       -6.4484915]\n",
      "SubSGD iter. 51/500: loss=39.06780585492639, w=[35.         -6.06995172]\n",
      "SubSGD iter. 52/500: loss=38.36780585492639, w=[35.7        -5.32396607]\n",
      "SubSGD iter. 53/500: loss=37.66851099649861, w=[36.4        -6.21279789]\n",
      "SubSGD iter. 54/500: loss=37.022234193481395, w=[37.1        -7.06664416]\n",
      "SubSGD iter. 55/500: loss=36.31093777799263, w=[37.8        -6.45365166]\n",
      "SubSGD iter. 56/500: loss=35.653682468672265, w=[38.5        -6.69767849]\n",
      "SubSGD iter. 57/500: loss=36.271711220168136, w=[37.8        -5.51367629]\n",
      "SubSGD iter. 58/500: loss=35.58246739507443, w=[38.5        -5.42514877]\n",
      "SubSGD iter. 59/500: loss=34.867805854926374, w=[39.2        -4.30001881]\n",
      "SubSGD iter. 60/500: loss=34.16780585492637, w=[39.9        -3.68702632]\n",
      "SubSGD iter. 61/500: loss=33.46780585492636, w=[40.6       -3.1589705]\n",
      "SubSGD iter. 62/500: loss=32.767805854926365, w=[41.3        -2.90868938]\n",
      "SubSGD iter. 63/500: loss=32.067805854926355, w=[42.         -2.05246733]\n",
      "SubSGD iter. 64/500: loss=31.367805854926353, w=[42.7        -1.52276636]\n",
      "SubSGD iter. 65/500: loss=30.672173016267738, w=[43.4       -2.1070789]\n",
      "SubSGD iter. 66/500: loss=29.977291064425085, w=[44.1        -1.99325857]\n",
      "SubSGD iter. 67/500: loss=29.327692725567513, w=[44.8        -2.61239972]\n",
      "SubSGD iter. 68/500: loss=28.72427995901203, w=[45.5        -3.30898448]\n",
      "SubSGD iter. 69/500: loss=28.0513721558065, w=[46.2        -3.10654759]\n",
      "SubSGD iter. 70/500: loss=27.34762115694878, w=[46.9        -2.61092133]\n",
      "SubSGD iter. 71/500: loss=26.681950568951414, w=[47.6       -2.4325692]\n",
      "SubSGD iter. 72/500: loss=26.167409524825523, w=[48.3        -3.21680193]\n",
      "SubSGD iter. 73/500: loss=25.430710633908426, w=[49.         -2.49863861]\n",
      "SubSGD iter. 74/500: loss=24.759442859376893, w=[49.7        -2.16341055]\n",
      "SubSGD iter. 75/500: loss=24.075454954256706, w=[50.4       -1.7398693]\n",
      "SubSGD iter. 76/500: loss=23.354908477531033, w=[51.1        -1.12520832]\n",
      "SubSGD iter. 77/500: loss=22.73500408649899, w=[51.8       -1.0366808]\n",
      "SubSGD iter. 78/500: loss=23.195676056196636, w=[51.1        -0.14784898]\n",
      "SubSGD iter. 79/500: loss=22.432382116793768, w=[51.8         0.79906328]\n",
      "SubSGD iter. 80/500: loss=21.71105777338409, w=[52.5         1.44521051]\n",
      "SubSGD iter. 81/500: loss=21.1506209285286, w=[53.2         0.96716231]\n",
      "SubSGD iter. 82/500: loss=20.70490110702933, w=[53.9         0.17279297]\n",
      "SubSGD iter. 83/500: loss=20.096631804633788, w=[54.6         0.30511649]\n",
      "SubSGD iter. 84/500: loss=19.3548969077649, w=[55.3         1.02327982]\n",
      "SubSGD iter. 85/500: loss=18.750105643914605, w=[56.          1.21962441]\n",
      "SubSGD iter. 86/500: loss=18.35046403373557, w=[56.7         0.79638499]\n",
      "SubSGD iter. 87/500: loss=17.839223714834123, w=[57.4         0.84854533]\n",
      "SubSGD iter. 88/500: loss=17.409084032091336, w=[58.1         0.73837122]\n",
      "SubSGD iter. 89/500: loss=16.735304868175703, w=[58.8         1.35228563]\n",
      "SubSGD iter. 90/500: loss=16.100964947834072, w=[59.5         1.88034144]\n",
      "SubSGD iter. 91/500: loss=15.631196531483415, w=[60.2        2.0461477]\n",
      "SubSGD iter. 92/500: loss=15.137624276471042, w=[60.9         2.31163141]\n",
      "SubSGD iter. 93/500: loss=14.576538419495863, w=[61.6         2.75155585]\n",
      "SubSGD iter. 94/500: loss=14.408825333203804, w=[62.3         2.38128776]\n",
      "SubSGD iter. 95/500: loss=13.751241002927294, w=[63.          3.08222176]\n",
      "SubSGD iter. 96/500: loss=13.467117984672011, w=[62.3         4.43617865]\n",
      "SubSGD iter. 97/500: loss=13.00887601504391, w=[63.          4.66530034]\n",
      "SubSGD iter. 98/500: loss=12.387192943118901, w=[63.7         5.28443374]\n",
      "SubSGD iter. 99/500: loss=11.781919877795215, w=[64.4        5.8767517]\n",
      "SubSGD iter. 100/500: loss=11.539336382315488, w=[65.1         5.72629502]\n",
      "SubSGD iter. 101/500: loss=11.693727716410509, w=[64.4         6.07337901]\n",
      "SubSGD iter. 102/500: loss=13.181474947553095, w=[65.1         2.70023533]\n",
      "SubSGD iter. 103/500: loss=12.55857748490658, w=[65.8         3.40116933]\n",
      "SubSGD iter. 104/500: loss=12.459081367748137, w=[66.5         3.20041979]\n",
      "SubSGD iter. 105/500: loss=11.931154634511955, w=[67.2         3.75362571]\n",
      "SubSGD iter. 106/500: loss=11.820485438026271, w=[66.5         4.31174941]\n",
      "SubSGD iter. 107/500: loss=11.9116894364532, w=[67.2         3.78711649]\n",
      "SubSGD iter. 108/500: loss=12.045365274785041, w=[67.9         3.25992785]\n",
      "SubSGD iter. 109/500: loss=11.507845969329308, w=[68.6         3.87384226]\n",
      "SubSGD iter. 110/500: loss=10.831533792226256, w=[69.3         4.73006431]\n",
      "SubSGD iter. 111/500: loss=10.305573691745856, w=[70.          5.37621155]\n",
      "SubSGD iter. 112/500: loss=10.216007239477012, w=[69.3         5.74647964]\n",
      "SubSGD iter. 113/500: loss=10.05501846164384, w=[70.          5.78376695]\n",
      "SubSGD iter. 114/500: loss=9.78006841722322, w=[70.7         6.02807967]\n",
      "SubSGD iter. 115/500: loss=9.091814311275819, w=[71.4         6.98621476]\n",
      "SubSGD iter. 116/500: loss=8.793038092431795, w=[70.7         7.68279952]\n",
      "SubSGD iter. 117/500: loss=8.726889275125217, w=[70.          8.06808366]\n",
      "SubSGD iter. 118/500: loss=8.468974719084375, w=[70.7         8.26460374]\n",
      "SubSGD iter. 119/500: loss=8.255470266223353, w=[71.4         8.40793433]\n",
      "SubSGD iter. 120/500: loss=7.8990682752162185, w=[70.7         9.30956659]\n",
      "SubSGD iter. 121/500: loss=7.363001941481913, w=[71.4        10.04384234]\n",
      "SubSGD iter. 122/500: loss=7.328613569620448, w=[70.7        10.42912648]\n",
      "SubSGD iter. 123/500: loss=6.902329894472246, w=[70.         11.78308337]\n",
      "SubSGD iter. 124/500: loss=6.988523360828486, w=[69.3        12.13980889]\n",
      "SubSGD iter. 125/500: loss=6.931232397480021, w=[70.        11.7094773]\n",
      "SubSGD iter. 126/500: loss=6.72674601013645, w=[70.7        11.72478497]\n",
      "SubSGD iter. 127/500: loss=6.752765448416926, w=[71.4        11.29445338]\n",
      "SubSGD iter. 128/500: loss=6.571445463126482, w=[72.1        11.42677691]\n",
      "SubSGD iter. 129/500: loss=6.090751799681705, w=[72.8        12.30594922]\n",
      "SubSGD iter. 130/500: loss=5.81085268063994, w=[73.5        12.95209645]\n",
      "SubSGD iter. 131/500: loss=5.816342246097461, w=[72.8        12.97739422]\n",
      "SubSGD iter. 132/500: loss=5.59469612006996, w=[73.5        13.62945111]\n",
      "SubSGD iter. 133/500: loss=5.58259060945559, w=[74.2        14.09603324]\n",
      "SubSGD iter. 134/500: loss=5.717549380551143, w=[74.9        14.29237783]\n",
      "SubSGD iter. 135/500: loss=5.897659705685988, w=[75.6        14.56251628]\n",
      "SubSGD iter. 136/500: loss=5.690199873570443, w=[74.9        14.52018973]\n",
      "SubSGD iter. 137/500: loss=5.47991460251955, w=[74.2        15.09014985]\n",
      "SubSGD iter. 138/500: loss=5.6207632678270345, w=[74.9       15.5403296]\n",
      "SubSGD iter. 139/500: loss=5.467054147455653, w=[74.2        15.53390172]\n",
      "SubSGD iter. 140/500: loss=5.361509647489059, w=[73.5        15.88423865]\n",
      "SubSGD iter. 141/500: loss=5.479087074719809, w=[74.2        15.11341558]\n",
      "SubSGD iter. 142/500: loss=5.666390141459571, w=[74.9        14.76029983]\n",
      "SubSGD iter. 143/500: loss=5.483601344119856, w=[74.2        15.00432666]\n",
      "SubSGD iter. 144/500: loss=5.6678214283819015, w=[74.9        16.36969611]\n",
      "SubSGD iter. 145/500: loss=5.946565379519973, w=[75.6        16.63517981]\n",
      "SubSGD iter. 146/500: loss=5.697638817880984, w=[74.9        16.66047758]\n",
      "SubSGD iter. 147/500: loss=5.4677464238713815, w=[74.2        15.49214678]\n",
      "SubSGD iter. 148/500: loss=5.633649806476408, w=[74.9        15.93207122]\n",
      "SubSGD iter. 149/500: loss=5.935707435818241, w=[75.6        16.54845817]\n",
      "SubSGD iter. 150/500: loss=5.6316005434614125, w=[74.9        15.89127547]\n",
      "SubSGD iter. 151/500: loss=6.003246821617684, w=[75.6        17.01640543]\n",
      "SubSGD iter. 152/500: loss=6.23493045635388, w=[76.3        16.46229835]\n",
      "SubSGD iter. 153/500: loss=5.870925916756432, w=[75.6        15.91390385]\n",
      "SubSGD iter. 154/500: loss=5.625004502061964, w=[74.9        15.71146696]\n",
      "SubSGD iter. 155/500: loss=5.482798397499717, w=[74.2        16.12254214]\n",
      "SubSGD iter. 156/500: loss=5.627208089428635, w=[74.9        15.78617918]\n",
      "SubSGD iter. 157/500: loss=5.494595479504762, w=[74.2       16.2946803]\n",
      "SubSGD iter. 158/500: loss=5.722206352927594, w=[74.9        16.84788622]\n",
      "SubSGD iter. 159/500: loss=5.498820113778563, w=[74.2        16.35225996]\n",
      "SubSGD iter. 160/500: loss=5.44747391854534, w=[73.5        16.97504637]\n",
      "SubSGD iter. 161/500: loss=5.366519327347932, w=[72.8        16.79669424]\n",
      "SubSGD iter. 162/500: loss=5.391724845731332, w=[72.1        16.91658606]\n",
      "SubSGD iter. 163/500: loss=5.48530695395524, w=[72.8        14.14628636]\n",
      "SubSGD iter. 164/500: loss=5.407762952430997, w=[72.1        14.98770487]\n",
      "SubSGD iter. 165/500: loss=5.438969198265161, w=[71.4        15.80636324]\n",
      "SubSGD iter. 166/500: loss=5.37947360648744, w=[72.1        16.81904158]\n",
      "SubSGD iter. 167/500: loss=5.470610714821968, w=[72.8        17.47109848]\n",
      "SubSGD iter. 168/500: loss=5.528599117072942, w=[73.5        17.43640762]\n",
      "SubSGD iter. 169/500: loss=5.675061920406846, w=[74.2        17.58930138]\n",
      "SubSGD iter. 170/500: loss=5.716119818346775, w=[74.9        16.80506865]\n",
      "SubSGD iter. 171/500: loss=6.008861045865092, w=[75.6        17.04938137]\n",
      "SubSGD iter. 172/500: loss=6.168800690476086, w=[76.3        15.86537917]\n",
      "SubSGD iter. 173/500: loss=6.597280393475041, w=[77.         16.41858509]\n",
      "SubSGD iter. 174/500: loss=7.023254542730632, w=[77.7        16.57147884]\n",
      "SubSGD iter. 175/500: loss=7.374082721862705, w=[78.4        15.83214605]\n",
      "SubSGD iter. 176/500: loss=6.969128976032463, w=[77.7        16.17923003]\n",
      "SubSGD iter. 177/500: loss=6.5693317911704225, w=[77.         16.19780949]\n",
      "SubSGD iter. 178/500: loss=6.153782145966017, w=[76.3        15.57867609]\n",
      "SubSGD iter. 179/500: loss=5.902070676838046, w=[75.6        14.52227486]\n",
      "SubSGD iter. 180/500: loss=5.623423907636269, w=[74.9        15.64820728]\n",
      "SubSGD iter. 181/500: loss=5.517228924395303, w=[74.2        16.54983954]\n",
      "SubSGD iter. 182/500: loss=5.487273750043377, w=[73.5        17.20672051]\n",
      "SubSGD iter. 183/500: loss=5.605039869918828, w=[72.8        18.09555233]\n",
      "SubSGD iter. 184/500: loss=5.471601674702513, w=[73.5        17.11875485]\n",
      "SubSGD iter. 185/500: loss=5.509155222962978, w=[72.8        17.67687856]\n",
      "SubSGD iter. 186/500: loss=5.508692322367452, w=[73.5        17.32694063]\n",
      "SubSGD iter. 187/500: loss=5.563597773657887, w=[74.2        16.97700269]\n",
      "SubSGD iter. 188/500: loss=5.549634668975337, w=[73.5        17.54696282]\n",
      "SubSGD iter. 189/500: loss=5.595106793941057, w=[72.8        18.05546394]\n",
      "SubSGD iter. 190/500: loss=5.7816835221448795, w=[72.1        18.61358764]\n",
      "SubSGD iter. 191/500: loss=5.602776710220813, w=[72.8       18.0866306]\n",
      "SubSGD iter. 192/500: loss=5.456949846823716, w=[72.1        17.34064495]\n",
      "SubSGD iter. 193/500: loss=5.494979302971929, w=[71.4        17.11152326]\n",
      "SubSGD iter. 194/500: loss=5.534537597220333, w=[72.1        17.72791021]\n",
      "SubSGD iter. 195/500: loss=5.525476880029938, w=[72.8        17.75428286]\n",
      "SubSGD iter. 196/500: loss=5.842294995028708, w=[73.5       18.7669612]\n",
      "SubSGD iter. 197/500: loss=6.094418150528343, w=[72.8        19.60300006]\n",
      "SubSGD iter. 198/500: loss=6.183103680618371, w=[73.5        19.69152757]\n",
      "SubSGD iter. 199/500: loss=5.980906030155559, w=[74.2        18.78989531]\n",
      "SubSGD iter. 200/500: loss=6.164838467834787, w=[74.9        18.68961821]\n",
      "SubSGD iter. 201/500: loss=6.466730658738864, w=[75.6        18.84251196]\n",
      "SubSGD iter. 202/500: loss=6.1827456460543795, w=[76.3        16.07221227]\n",
      "SubSGD iter. 203/500: loss=6.003129913672671, w=[75.6        17.01571875]\n",
      "SubSGD iter. 204/500: loss=5.679006269515994, w=[74.9        16.48601778]\n",
      "SubSGD iter. 205/500: loss=5.478829816846997, w=[74.2        15.12064832]\n",
      "SubSGD iter. 206/500: loss=5.361165373915853, w=[73.5        15.91501767]\n",
      "SubSGD iter. 207/500: loss=5.3271393451830225, w=[72.8        16.47314137]\n",
      "SubSGD iter. 208/500: loss=5.559397239133715, w=[73.5        17.59827133]\n",
      "SubSGD iter. 209/500: loss=5.653338553342497, w=[72.8        18.27887051]\n",
      "SubSGD iter. 210/500: loss=5.759893068791745, w=[73.5       18.4813074]\n",
      "SubSGD iter. 211/500: loss=5.836611498845492, w=[72.8        18.89238258]\n",
      "SubSGD iter. 212/500: loss=5.6791018128090505, w=[72.1        18.27846817]\n",
      "SubSGD iter. 213/500: loss=5.384971719867844, w=[72.8        16.92451128]\n",
      "SubSGD iter. 214/500: loss=5.3380875550409685, w=[72.1        16.01933617]\n",
      "SubSGD iter. 215/500: loss=5.435570320010635, w=[72.8        17.25865247]\n",
      "SubSGD iter. 216/500: loss=5.362758616741762, w=[72.1        16.65412037]\n",
      "SubSGD iter. 217/500: loss=5.486307068571027, w=[71.4        17.04955716]\n",
      "SubSGD iter. 218/500: loss=5.520690429195738, w=[72.1        17.66347157]\n",
      "SubSGD iter. 219/500: loss=5.500118139551582, w=[72.8        17.63104257]\n",
      "SubSGD iter. 220/500: loss=5.573323450843393, w=[73.5        17.66832988]\n",
      "SubSGD iter. 221/500: loss=5.530685707992832, w=[72.8        17.77850399]\n",
      "SubSGD iter. 222/500: loss=5.495674155397809, w=[73.5        17.25387107]\n",
      "SubSGD iter. 223/500: loss=5.468423722901155, w=[72.8        17.45870459]\n",
      "SubSGD iter. 224/500: loss=5.678775533167807, w=[72.1        18.27736296]\n",
      "SubSGD iter. 225/500: loss=5.9423916642892545, w=[71.4        18.82933439]\n",
      "SubSGD iter. 226/500: loss=5.768871375159977, w=[72.1        18.57390251]\n",
      "SubSGD iter. 227/500: loss=5.823937354413942, w=[71.4        18.44157898]\n",
      "SubSGD iter. 228/500: loss=5.908321892926979, w=[70.7        18.17679388]\n",
      "SubSGD iter. 229/500: loss=5.76152392616477, w=[71.4        18.22895422]\n",
      "SubSGD iter. 230/500: loss=6.131981403751284, w=[70.7       18.9095534]\n",
      "SubSGD iter. 231/500: loss=5.927776192230296, w=[71.4        18.78253895]\n",
      "SubSGD iter. 232/500: loss=5.646293544546489, w=[72.1       18.1633978]\n",
      "SubSGD iter. 233/500: loss=5.498035095623871, w=[72.8        17.62038134]\n",
      "SubSGD iter. 234/500: loss=5.5969935297206765, w=[73.5       17.7861876]\n",
      "SubSGD iter. 235/500: loss=5.757183249118759, w=[74.2        17.95164039]\n",
      "SubSGD iter. 236/500: loss=5.7325733236947585, w=[73.5        18.38265107]\n",
      "SubSGD iter. 237/500: loss=5.716164983271571, w=[72.8        18.50254289]\n",
      "SubSGD iter. 238/500: loss=5.517541566791549, w=[73.5        17.37661047]\n",
      "SubSGD iter. 239/500: loss=5.539214490699988, w=[72.8        17.81680573]\n",
      "SubSGD iter. 240/500: loss=5.6287544814449415, w=[72.1        18.09935355]\n",
      "SubSGD iter. 241/500: loss=5.5210500313463, w=[72.8        17.73343282]\n",
      "SubSGD iter. 242/500: loss=5.4843689807691005, w=[73.5        17.19041635]\n",
      "SubSGD iter. 243/500: loss=5.482709496448189, w=[72.8        17.53750034]\n",
      "SubSGD iter. 244/500: loss=5.4400219758169435, w=[72.1        17.24900308]\n",
      "SubSGD iter. 245/500: loss=5.340113685810476, w=[72.8        16.59212211]\n",
      "SubSGD iter. 246/500: loss=5.376316904047082, w=[72.1        16.79287164]\n",
      "SubSGD iter. 247/500: loss=5.510460702089102, w=[71.4        17.20394683]\n",
      "SubSGD iter. 248/500: loss=5.452721011796923, w=[72.1        17.31776716]\n",
      "SubSGD iter. 249/500: loss=5.633763488274026, w=[71.4        17.75796241]\n",
      "SubSGD iter. 250/500: loss=5.658596236273533, w=[72.1        18.20814216]\n",
      "SubSGD iter. 251/500: loss=5.762798419215337, w=[71.4        18.23343992]\n",
      "SubSGD iter. 252/500: loss=5.857607498310683, w=[72.1       18.8481009]\n",
      "SubSGD iter. 253/500: loss=5.731442779133714, w=[71.4        18.12308071]\n",
      "SubSGD iter. 254/500: loss=5.682073293774297, w=[72.1        18.28853351]\n",
      "SubSGD iter. 255/500: loss=5.908534111834546, w=[71.4        18.72032608]\n",
      "SubSGD iter. 256/500: loss=5.6986666082716715, w=[72.1        18.34474025]\n",
      "SubSGD iter. 257/500: loss=5.458282576652513, w=[72.8        17.40123376]\n",
      "SubSGD iter. 258/500: loss=5.503236460319363, w=[73.5        14.02809008]\n",
      "SubSGD iter. 259/500: loss=5.570521415709866, w=[74.2        14.17142067]\n",
      "SubSGD iter. 260/500: loss=5.66036584599357, w=[74.9        14.82347757]\n",
      "SubSGD iter. 261/500: loss=5.506051423473975, w=[74.2        14.71317754]\n",
      "SubSGD iter. 262/500: loss=5.754938149484743, w=[74.9       14.0264829]\n",
      "SubSGD iter. 263/500: loss=5.569678831356726, w=[74.2        14.17693958]\n",
      "SubSGD iter. 264/500: loss=5.659839592375633, w=[74.9        14.82899648]\n",
      "SubSGD iter. 265/500: loss=5.851619153608263, w=[75.6        15.17346472]\n",
      "SubSGD iter. 266/500: loss=6.155263634488734, w=[76.3        15.61338917]\n",
      "SubSGD iter. 267/500: loss=5.849830595533204, w=[75.6        15.38426747]\n",
      "SubSGD iter. 268/500: loss=5.625616719323884, w=[74.9        15.73460441]\n",
      "SubSGD iter. 269/500: loss=5.485982759467606, w=[74.2        16.17429281]\n",
      "SubSGD iter. 270/500: loss=5.623459734117776, w=[74.9        15.64965989]\n",
      "SubSGD iter. 271/500: loss=5.4658624762177315, w=[74.2        15.77016444]\n",
      "SubSGD iter. 272/500: loss=5.378308847961498, w=[73.5        15.24210862]\n",
      "SubSGD iter. 273/500: loss=5.499919099461133, w=[74.2        16.36723858]\n",
      "SubSGD iter. 274/500: loss=5.413843012408549, w=[73.5        16.71241567]\n",
      "SubSGD iter. 275/500: loss=5.437425160969528, w=[72.8        17.27053938]\n",
      "SubSGD iter. 276/500: loss=5.58924905134345, w=[72.1        17.95113855]\n",
      "SubSGD iter. 277/500: loss=5.622384049612734, w=[72.8        18.16230295]\n",
      "SubSGD iter. 278/500: loss=5.601793599759994, w=[73.5       17.8091872]\n",
      "SubSGD iter. 279/500: loss=5.427144527790023, w=[72.8        17.20465509]\n",
      "SubSGD iter. 280/500: loss=5.459623958490541, w=[72.1        17.35511177]\n",
      "SubSGD iter. 281/500: loss=5.418659524438887, w=[72.8        17.15027825]\n",
      "SubSGD iter. 282/500: loss=5.38833122589401, w=[72.1        16.88956603]\n",
      "SubSGD iter. 283/500: loss=5.524056945189766, w=[71.4        17.27485017]\n",
      "SubSGD iter. 284/500: loss=5.610955436542549, w=[70.7        16.19097477]\n",
      "SubSGD iter. 285/500: loss=5.447539314151109, w=[71.4        16.53894182]\n",
      "SubSGD iter. 286/500: loss=5.426086428179011, w=[72.1        17.16242994]\n",
      "SubSGD iter. 287/500: loss=5.477718742716992, w=[72.8        17.51039698]\n",
      "SubSGD iter. 288/500: loss=5.559521518292083, w=[73.5       17.5989245]\n",
      "SubSGD iter. 289/500: loss=5.732282129772836, w=[74.2        17.84323722]\n",
      "SubSGD iter. 290/500: loss=5.6478897046433625, w=[73.5      18.027398]\n",
      "SubSGD iter. 291/500: loss=5.4816546442799465, w=[72.8        17.53177174]\n",
      "SubSGD iter. 292/500: loss=5.717862879502113, w=[73.5        18.32694146]\n",
      "SubSGD iter. 293/500: loss=5.69733273740313, w=[72.8        18.43711558]\n",
      "SubSGD iter. 294/500: loss=5.763416871019976, w=[72.1       18.5570074]\n",
      "SubSGD iter. 295/500: loss=5.8747488776862165, w=[72.8        19.00718714]\n",
      "SubSGD iter. 296/500: loss=6.029348501315984, w=[72.1        19.35427113]\n",
      "SubSGD iter. 297/500: loss=6.102774625101502, w=[72.8        19.62440958]\n",
      "SubSGD iter. 298/500: loss=5.899665698791082, w=[73.5        18.94606258]\n",
      "SubSGD iter. 299/500: loss=5.9542499394711035, w=[72.8        19.23123708]\n",
      "SubSGD iter. 300/500: loss=6.2466824637141904, w=[72.1        19.91183625]\n",
      "SubSGD iter. 301/500: loss=6.377519957520185, w=[72.8       20.2598033]\n",
      "SubSGD iter. 302/500: loss=6.634706311369601, w=[72.1        20.79532298]\n",
      "SubSGD iter. 303/500: loss=6.546609149664163, w=[72.8       20.6118572]\n",
      "SubSGD iter. 304/500: loss=6.264223438628637, w=[72.1       19.9546745]\n",
      "SubSGD iter. 305/500: loss=6.3961065401821315, w=[72.8        20.29914275]\n",
      "SubSGD iter. 306/500: loss=6.067239729232493, w=[72.1        19.45795258]\n",
      "SubSGD iter. 307/500: loss=6.031801650794108, w=[72.8        19.43937312]\n",
      "SubSGD iter. 308/500: loss=6.0580971541240505, w=[72.1        19.43294524]\n",
      "SubSGD iter. 309/500: loss=5.7244932327696505, w=[72.8        18.53131297]\n",
      "SubSGD iter. 310/500: loss=5.830286102397462, w=[73.5        18.72783306]\n",
      "SubSGD iter. 311/500: loss=5.9659385741640785, w=[72.8        19.26335274]\n",
      "SubSGD iter. 312/500: loss=6.155414897978381, w=[72.1        19.68659217]\n",
      "SubSGD iter. 313/500: loss=6.028685130776023, w=[72.8        19.43116028]\n",
      "SubSGD iter. 314/500: loss=6.03685803088995, w=[72.1        19.37485067]\n",
      "SubSGD iter. 315/500: loss=5.747646657793537, w=[71.4        18.18011183]\n",
      "SubSGD iter. 316/500: loss=5.913021526943873, w=[70.7        18.19386703]\n",
      "SubSGD iter. 317/500: loss=5.5621181583922334, w=[71.4       17.4597527]\n",
      "SubSGD iter. 318/500: loss=5.626116204019608, w=[70.7        16.61856254]\n",
      "SubSGD iter. 319/500: loss=5.43418562690185, w=[71.4        16.09392961]\n",
      "SubSGD iter. 320/500: loss=5.341722484032807, w=[72.1       16.2372602]\n",
      "SubSGD iter. 321/500: loss=5.453597645861107, w=[71.4        15.50298446]\n",
      "SubSGD iter. 322/500: loss=5.6122588793116766, w=[70.7        16.05014676]\n",
      "SubSGD iter. 323/500: loss=5.916999389700179, w=[70.         16.78426108]\n",
      "SubSGD iter. 324/500: loss=5.775106201572001, w=[70.7        17.61759867]\n",
      "SubSGD iter. 325/500: loss=5.488157139149649, w=[71.4        17.06349159]\n",
      "SubSGD iter. 326/500: loss=5.672154778133428, w=[70.7        17.02116504]\n",
      "SubSGD iter. 327/500: loss=5.871719824274249, w=[70.         16.27517938]\n",
      "SubSGD iter. 328/500: loss=5.620845632099357, w=[70.7       15.7021693]\n",
      "SubSGD iter. 329/500: loss=5.982738374059576, w=[70.         14.74403421]\n",
      "SubSGD iter. 330/500: loss=6.244025832006858, w=[69.3       15.0892113]\n",
      "SubSGD iter. 331/500: loss=5.859993370552734, w=[70.         15.64241721]\n",
      "SubSGD iter. 332/500: loss=5.6186499648540575, w=[70.7        15.75271725]\n",
      "SubSGD iter. 333/500: loss=5.874243913667587, w=[70.         15.49200502]\n",
      "SubSGD iter. 334/500: loss=6.336082724444202, w=[70.7        12.72170533]\n",
      "SubSGD iter. 335/500: loss=6.431085438239136, w=[70.         13.07204226]\n",
      "SubSGD iter. 336/500: loss=6.550658178193591, w=[69.3       13.5989993]\n",
      "SubSGD iter. 337/500: loss=6.23037118890997, w=[70.         13.71281962]\n",
      "SubSGD iter. 338/500: loss=5.828083843115349, w=[70.7        14.32673404]\n",
      "SubSGD iter. 339/500: loss=5.520861088539059, w=[71.4        14.85091459]\n",
      "SubSGD iter. 340/500: loss=5.344753573013324, w=[72.1        15.70713664]\n",
      "SubSGD iter. 341/500: loss=5.4531808167191755, w=[71.4        15.50837578]\n",
      "SubSGD iter. 342/500: loss=5.611324917408231, w=[70.7        16.12117395]\n",
      "SubSGD iter. 343/500: loss=5.438524186809222, w=[71.4        16.31769403]\n",
      "SubSGD iter. 344/500: loss=5.637247295192893, w=[70.7        16.74093345]\n",
      "SubSGD iter. 345/500: loss=5.449776697019599, w=[71.4        15.55693125]\n",
      "SubSGD iter. 346/500: loss=5.611405378188255, w=[70.7        16.11505496]\n",
      "SubSGD iter. 347/500: loss=5.51583655870028, w=[71.4        17.23336518]\n",
      "SubSGD iter. 348/500: loss=5.611236184380792, w=[70.7        16.28645292]\n",
      "SubSGD iter. 349/500: loss=5.4581918083394, w=[71.4        16.73511195]\n",
      "SubSGD iter. 350/500: loss=5.362757227480007, w=[72.1        15.38115506]\n",
      "SubSGD iter. 351/500: loss=5.451384330327525, w=[71.4        15.53161174]\n",
      "SubSGD iter. 352/500: loss=5.338501005371512, w=[72.1        16.07256879]\n",
      "SubSGD iter. 353/500: loss=5.445805577393489, w=[71.4        16.50436137]\n",
      "SubSGD iter. 354/500: loss=5.632333099159767, w=[70.7        16.68852215]\n",
      "SubSGD iter. 355/500: loss=5.434229989374936, w=[71.4        16.14550568]\n",
      "SubSGD iter. 356/500: loss=5.372840878675334, w=[72.1        16.76016667]\n",
      "SubSGD iter. 357/500: loss=5.501753915096578, w=[72.8        17.63941463]\n",
      "SubSGD iter. 358/500: loss=5.61571668159211, w=[72.1        18.05048982]\n",
      "SubSGD iter. 359/500: loss=5.5617755034166985, w=[71.4        17.45817185]\n",
      "SubSGD iter. 360/500: loss=5.7004957428920315, w=[70.7        17.20789074]\n",
      "SubSGD iter. 361/500: loss=5.4612547181827065, w=[71.4        16.78210459]\n",
      "SubSGD iter. 362/500: loss=5.467323436055613, w=[72.1        17.39676557]\n",
      "SubSGD iter. 363/500: loss=5.738362894406725, w=[72.8        18.57757027]\n",
      "SubSGD iter. 364/500: loss=5.848977531568065, w=[73.5        18.78873467]\n",
      "SubSGD iter. 365/500: loss=5.807711307725472, w=[72.8        18.80248986]\n",
      "SubSGD iter. 366/500: loss=5.700803969503623, w=[73.5       18.2594734]\n",
      "SubSGD iter. 367/500: loss=5.602097037846588, w=[74.2        17.22195921]\n",
      "SubSGD iter. 368/500: loss=5.450287231096206, w=[73.5        16.99283752]\n",
      "SubSGD iter. 369/500: loss=5.319459520430914, w=[72.8        16.36167218]\n",
      "SubSGD iter. 370/500: loss=5.379771560606142, w=[73.5        16.39895949]\n",
      "SubSGD iter. 371/500: loss=5.3851808748998735, w=[72.8        16.92591652]\n",
      "SubSGD iter. 372/500: loss=5.395993465056038, w=[73.5        16.55033068]\n",
      "SubSGD iter. 373/500: loss=5.319154522591028, w=[72.8        16.35156982]\n",
      "SubSGD iter. 374/500: loss=5.33898288764583, w=[72.1        16.12244813]\n",
      "SubSGD iter. 375/500: loss=5.434143590775438, w=[71.4        16.11602025]\n",
      "SubSGD iter. 376/500: loss=5.611019390903181, w=[70.7        16.26647692]\n",
      "SubSGD iter. 377/500: loss=5.853529178992546, w=[70.         15.93124887]\n",
      "SubSGD iter. 378/500: loss=5.611119671935882, w=[70.7        16.27571711]\n",
      "SubSGD iter. 379/500: loss=5.873859586594585, w=[70.         16.30101488]\n",
      "SubSGD iter. 380/500: loss=5.6142263140345365, w=[70.7        15.90503864]\n",
      "SubSGD iter. 381/500: loss=5.463503514865455, w=[71.4     15.37785]\n",
      "SubSGD iter. 382/500: loss=5.6317569044507, w=[70.7        15.49774181]\n",
      "SubSGD iter. 383/500: loss=5.44143186737588, w=[71.4        15.74205453]\n",
      "SubSGD iter. 384/500: loss=5.611385141195324, w=[70.7        16.30017824]\n",
      "SubSGD iter. 385/500: loss=5.483994973107295, w=[71.4        17.03214283]\n",
      "SubSGD iter. 386/500: loss=5.359158429913217, w=[72.1        16.60181124]\n",
      "SubSGD iter. 387/500: loss=5.544811609670306, w=[72.8        17.84112754]\n",
      "SubSGD iter. 388/500: loss=5.626935522068667, w=[73.5        17.92965506]\n",
      "SubSGD iter. 389/500: loss=5.430899799739463, w=[72.8        17.22872106]\n",
      "SubSGD iter. 390/500: loss=5.3777326170045034, w=[72.1        16.80517981]\n",
      "SubSGD iter. 391/500: loss=5.323248911483798, w=[72.8        16.43491172]\n",
      "SubSGD iter. 392/500: loss=5.363031094077175, w=[73.5        15.74821708]\n",
      "SubSGD iter. 393/500: loss=5.322870914181795, w=[72.8        16.42881626]\n",
      "SubSGD iter. 394/500: loss=5.342012589089374, w=[72.1        15.82341311]\n",
      "SubSGD iter. 395/500: loss=5.32342721733341, w=[72.8        16.43732752]\n",
      "SubSGD iter. 396/500: loss=5.3449814719497475, w=[72.1        16.32702749]\n",
      "SubSGD iter. 397/500: loss=5.454423974445276, w=[71.4        16.67220458]\n",
      "SubSGD iter. 398/500: loss=5.613734473075389, w=[70.7        15.93792883]\n",
      "SubSGD iter. 399/500: loss=5.963416568185902, w=[70.         14.85405344]\n",
      "SubSGD iter. 400/500: loss=5.63684522102844, w=[70.7        15.40725935]\n",
      "SubSGD iter. 401/500: loss=5.435981122175379, w=[71.4        15.96046527]\n",
      "SubSGD iter. 402/500: loss=5.338117113571305, w=[72.1        15.99775258]\n",
      "SubSGD iter. 403/500: loss=5.334289283539998, w=[72.8        16.53870963]\n",
      "SubSGD iter. 404/500: loss=5.340459479551567, w=[72.1        16.19499701]\n",
      "SubSGD iter. 405/500: loss=5.450140441203585, w=[71.4       16.5904338]\n",
      "SubSGD iter. 406/500: loss=5.637277778348364, w=[72.1        18.13049614]\n",
      "SubSGD iter. 407/500: loss=5.430922084707879, w=[72.8        17.22886388]\n",
      "SubSGD iter. 408/500: loss=5.535595222088437, w=[73.5       17.4731766]\n",
      "SubSGD iter. 409/500: loss=5.424027495880018, w=[72.8        17.18467933]\n",
      "SubSGD iter. 410/500: loss=5.4247009965624695, w=[73.5       16.8090935]\n",
      "SubSGD iter. 411/500: loss=5.574559105757699, w=[74.2        17.05340622]\n",
      "SubSGD iter. 412/500: loss=5.391002730927127, w=[73.5        16.50501172]\n",
      "SubSGD iter. 413/500: loss=5.427747086023869, w=[72.8        17.20851663]\n",
      "SubSGD iter. 414/500: loss=5.4246971012774825, w=[73.5        16.80905886]\n",
      "SubSGD iter. 415/500: loss=5.4738999179420915, w=[72.8        17.48965804]\n",
      "SubSGD iter. 416/500: loss=5.708741166053662, w=[72.1        18.37848986]\n",
      "SubSGD iter. 417/500: loss=5.480342082792323, w=[72.8        17.52464359]\n",
      "SubSGD iter. 418/500: loss=5.436174851519044, w=[73.5        16.90359202]\n",
      "SubSGD iter. 419/500: loss=5.466085018135875, w=[74.2        15.77765961]\n",
      "SubSGD iter. 420/500: loss=5.370867831107497, w=[73.5        16.28616072]\n",
      "SubSGD iter. 421/500: loss=5.352975745575404, w=[72.8        15.09142189]\n",
      "SubSGD iter. 422/500: loss=5.537003835710788, w=[72.1        14.21169176]\n",
      "SubSGD iter. 423/500: loss=5.558377596650295, w=[72.8        13.81571551]\n",
      "SubSGD iter. 424/500: loss=5.505107895315384, w=[73.5       14.0181524]\n",
      "SubSGD iter. 425/500: loss=5.446544455088454, w=[72.8        14.36332949]\n",
      "SubSGD iter. 426/500: loss=5.447836868231729, w=[72.1        14.71041348]\n",
      "SubSGD iter. 427/500: loss=5.397852233658017, w=[72.8        14.69183402]\n",
      "SubSGD iter. 428/500: loss=5.361639396582881, w=[73.5        15.87263872]\n",
      "SubSGD iter. 429/500: loss=5.475482069893124, w=[74.2        15.98645905]\n",
      "SubSGD iter. 430/500: loss=5.744278422174413, w=[74.9        16.99913739]\n",
      "SubSGD iter. 431/500: loss=5.507424543094118, w=[74.2        16.45074288]\n",
      "SubSGD iter. 432/500: loss=5.510943226872646, w=[73.5        17.33957471]\n",
      "SubSGD iter. 433/500: loss=5.7619216330837535, w=[74.2        17.97184914]\n",
      "SubSGD iter. 434/500: loss=5.758660804385488, w=[73.5        18.47686687]\n",
      "SubSGD iter. 435/500: loss=5.5456089773899375, w=[72.8        17.84459244]\n",
      "SubSGD iter. 436/500: loss=5.533557382260396, w=[73.5        17.46246646]\n",
      "SubSGD iter. 437/500: loss=5.521159125173021, w=[72.8        17.73394664]\n",
      "SubSGD iter. 438/500: loss=5.67742500529856, w=[73.5        18.16146372]\n",
      "SubSGD iter. 439/500: loss=5.871140505194982, w=[74.2        18.41106219]\n",
      "SubSGD iter. 440/500: loss=6.087914407776639, w=[74.9       18.4483495]\n",
      "SubSGD iter. 441/500: loss=5.790897040016585, w=[74.2        18.09332202]\n",
      "SubSGD iter. 442/500: loss=5.823536778581589, w=[74.9        17.47418086]\n",
      "SubSGD iter. 443/500: loss=5.533354200358042, w=[74.2        16.71282391]\n",
      "SubSGD iter. 444/500: loss=5.632632115297216, w=[74.9        15.91299318]\n",
      "SubSGD iter. 445/500: loss=5.53733117732298, w=[74.2        16.75302075]\n",
      "SubSGD iter. 446/500: loss=5.670674091567903, w=[74.9      16.399905]\n",
      "SubSGD iter. 447/500: loss=5.469049542764963, w=[74.2        15.44176991]\n",
      "SubSGD iter. 448/500: loss=5.361629063941041, w=[73.5        15.87356249]\n",
      "SubSGD iter. 449/500: loss=5.31305169124467, w=[72.8        16.02401916]\n",
      "SubSGD iter. 450/500: loss=5.341336835320309, w=[72.1       16.2247687]\n",
      "SubSGD iter. 451/500: loss=5.341333512272935, w=[72.8        16.60330848]\n",
      "SubSGD iter. 452/500: loss=5.341213235566072, w=[72.1        15.85732283]\n",
      "SubSGD iter. 453/500: loss=5.437492362974712, w=[71.4        16.28056225]\n",
      "SubSGD iter. 454/500: loss=5.345527486837601, w=[72.1        15.67679618]\n",
      "SubSGD iter. 455/500: loss=5.317814961534859, w=[72.8        16.30028431]\n",
      "SubSGD iter. 456/500: loss=5.392539295514897, w=[72.1        16.92307071]\n",
      "SubSGD iter. 457/500: loss=5.437609161077549, w=[71.4        15.86666948]\n",
      "SubSGD iter. 458/500: loss=5.613933966853084, w=[70.7        16.43662961]\n",
      "SubSGD iter. 459/500: loss=5.477720327183934, w=[71.4        16.97758666]\n",
      "SubSGD iter. 460/500: loss=5.385548827105543, w=[72.1        16.86741254]\n",
      "SubSGD iter. 461/500: loss=5.472252925193563, w=[72.8        17.48040504]\n",
      "SubSGD iter. 462/500: loss=5.4101691854506635, w=[72.1        17.05686379]\n",
      "SubSGD iter. 463/500: loss=5.438982086081689, w=[71.4       16.3318436]\n",
      "SubSGD iter. 464/500: loss=5.342454774772094, w=[72.1        15.80465496]\n",
      "SubSGD iter. 465/500: loss=5.32435249455341, w=[72.8        16.44758466]\n",
      "SubSGD iter. 466/500: loss=5.365742437145699, w=[73.5        16.20660474]\n",
      "SubSGD iter. 467/500: loss=5.469575345586578, w=[74.2        15.85666681]\n",
      "SubSGD iter. 468/500: loss=5.405215530920252, w=[73.5        14.80026558]\n",
      "SubSGD iter. 469/500: loss=5.501414413926005, w=[74.2        14.76557472]\n",
      "SubSGD iter. 470/500: loss=5.797889754987055, w=[74.9        13.73864252]\n",
      "SubSGD iter. 471/500: loss=5.938601551483405, w=[75.6        14.20522465]\n",
      "SubSGD iter. 472/500: loss=6.2225925452301825, w=[76.3        13.94979277]\n",
      "SubSGD iter. 473/500: loss=6.579635775776967, w=[77.         13.61342981]\n",
      "SubSGD iter. 474/500: loss=6.356066470925061, w=[76.3        13.16477077]\n",
      "SubSGD iter. 475/500: loss=6.700499162549119, w=[77.         12.92379086]\n",
      "SubSGD iter. 476/500: loss=6.565366340254059, w=[76.3        12.22285686]\n",
      "SubSGD iter. 477/500: loss=6.216120365654122, w=[75.6        12.64864301]\n",
      "SubSGD iter. 478/500: loss=5.970135956585126, w=[74.9        12.92012319]\n",
      "SubSGD iter. 479/500: loss=6.063308229170313, w=[75.6        13.37030293]\n",
      "SubSGD iter. 480/500: loss=5.769063353789004, w=[74.9        13.92842664]\n",
      "SubSGD iter. 481/500: loss=5.855293549050432, w=[75.6        15.04673686]\n",
      "SubSGD iter. 482/500: loss=5.686954745538894, w=[74.9       14.5511106]\n",
      "SubSGD iter. 483/500: loss=5.471391068575754, w=[74.2        15.35922434]\n",
      "SubSGD iter. 484/500: loss=5.6496527892716815, w=[74.9        16.15439406]\n",
      "SubSGD iter. 485/500: loss=5.9398959008320515, w=[75.6        16.58191115]\n",
      "SubSGD iter. 486/500: loss=5.775092568421848, w=[74.9        17.19470932]\n",
      "SubSGD iter. 487/500: loss=5.507224766603705, w=[74.2        16.44872367]\n",
      "SubSGD iter. 488/500: loss=5.623575515214195, w=[74.9        15.65435432]\n",
      "SubSGD iter. 489/500: loss=5.470118864879887, w=[74.2       15.4040732]\n",
      "SubSGD iter. 490/500: loss=5.671425955560617, w=[74.9        14.70748845]\n",
      "SubSGD iter. 491/500: loss=5.477886235971748, w=[74.2        15.14717686]\n",
      "SubSGD iter. 492/500: loss=5.626954399845578, w=[74.9        15.77945129]\n",
      "SubSGD iter. 493/500: loss=5.487659983916339, w=[74.2        14.93826112]\n",
      "SubSGD iter. 494/500: loss=5.44407470014148, w=[73.5       14.4102053]\n",
      "SubSGD iter. 495/500: loss=5.362775692311065, w=[72.8        14.98016543]\n",
      "SubSGD iter. 496/500: loss=5.377930478122846, w=[73.5        15.25030388]\n",
      "SubSGD iter. 497/500: loss=5.457248971776724, w=[72.8        14.30339162]\n",
      "SubSGD iter. 498/500: loss=5.39221509530444, w=[73.5        14.96653593]\n",
      "SubSGD iter. 499/500: loss=5.329546917537514, w=[72.8        15.47503705]\n",
      "SubSGD iter. 500/500: loss=5.397212052508317, w=[73.5        14.90202697]\n",
      "SubSGD: execution time=0.027 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ba5f244ac64e0d8947e376961feb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
